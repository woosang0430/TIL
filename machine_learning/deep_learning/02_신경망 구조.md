# DNN(Deep Neural Network)
## 신경망 구성요소
- 딥러닝 프로세스
- ![image](https://user-images.githubusercontent.com/77317312/115013468-0c412b00-9eec-11eb-8473-da4fc65fd702.png)
- **층(Layer)** : Network를 구성하는 layer(층)
- **손실함수(loss function)** : 가중치를 어떻게 업데이트할 지 예측 결과와 Ground truth(실제타깃) 사이의 차이를 정의
- **optimizer** : 가중치를 업데이트하여 모델의 성능을 최적화

## 1. 유닛/노드/뉴런 (unit, node, neuron)
- Tensor를 입력받아 tensor를 출력하는 데이터 처리 모듈
  - input -> output
- 입력 값에 Weight(가중치)를 곱하고 bias(편향)을 더한 뒤 활성화 함수를 거쳐 출력한다.
- 하나의 노드 구성
- ![image](https://user-images.githubusercontent.com/77317312/115013922-a4d7ab00-9eec-11eb-86ba-cf34c3dff108.png)
  - input vector(입력값) : X = (x1, x2, x3)^T
  - Weights(가중치) : W = (w1, w2, w3)^T
  - Bias(편향) : b
  - ACtivation function(활성함수)
    - 활성함수로는 비선형 함수를 사용
    - ![image](https://user-images.githubusercontent.com/77317312/115014302-1dd70280-9eed-11eb-807f-2a95c0a614a9.png)

## 2. 레이어/층(Layer)
- **input layer(입력층)**
- **output Layer(출력층)** 
- **Hidden Layer(은닉층)**
- **Network(망)** : layer들의 연결
- 많이 사용되는 Layer의 예
  - **Fully connected layer(Dense layer)** : 거의 추론 단계에서 사용
  - **Convolution layer** : 이미지쪽에서 많이 사용
  - **Recurrent layer** : 시계열 연속된 데이터
  - **Embedding layer** : 텍스트를 다룰 떄
- Layers : https://www.tensorflow.org/api_docs/python/tf/keras/layers

## 3. 모델
- Layer를 쌓아 만드는 네트워크
- 이전 레이어의 출력을 input으로 받아 output을 주는 층을 순서대로 쌓음
- 적절한 network 구조(architecture)를 찾는 것은 과학 보다는 예술의 경지! 많은 경험이 필요
- 기존의 잘 작동한 구조를 기반으로 구현하는 방식으로 접근
- ![image](https://user-images.githubusercontent.com/77317312/115015222-4c091200-9eee-11eb-8c3d-82b3f58caa09.png)

## .번외 딥러닝(Deep learning)
- 신경망이 많아지면 깊은 딥러닝이라고 한다.
- ![image](https://user-images.githubusercontent.com/77317312/115015354-7955c000-9eee-11eb-8f05-a1541698a09e.png)

## 4. 손실함수(loss function, 비용함수)
- Model을 통해 나온 예측값(prediction) y^와 실제 데이터(output) y의 차이를 수령화하는 함수
- 훈련하는 동안 최소화될 값으로 이 값을 바탕으로 파라미터(가중치와 편향)을 업데이트 한다.
- 문제의 종류에 따라 다른 loss함수를 사용
### 4-1. 해결하려는 문제의 종류에 따라 표준적인 loss function이 존재함
- **Binary classification(이진 분류)**
  - binary_crossentropy를 loss function으로 사용
  - ![image](https://user-images.githubusercontent.com/77317312/115037462-73b9a380-9f09-11eb-84de-f0c4891af31e.png)

- **Multi-class classification(다중 클래스 분류)**
  - categorical_crossentropy를 loss function으로 사용
  - ![image](https://user-images.githubusercontent.com/77317312/115037721-bbd8c600-9f09-11eb-880f-4dd37fb2845e.png)

- **Regression(회귀)**
  - Mean squared error를 loss function으로 사용('mse'로 지정)
  - ![image](https://user-images.githubusercontent.com/77317312/115037881-eb87ce00-9f09-11eb-9054-ee0f286d7fe0.png)
- https://www.tensorflow.org/api_docs/python/tf/keras/losses

## 5. 평가지표(metrics)
- 모델의 성능을 평가하는 지표
- 손실함수와 차이
  - 손실함수는 모델을 학습할 때 가중치 업데이트를 위한 오차를 구할 때 사용
  - 평가지표 함수는 모델의 성능을 확인하는데 사용

## 6. 활성 함수(activation function)
- 각 유닛이 입력결과를 처리한 후 출력하기 위해 거치는 함수
- 같은 층(layer)의 모든 유닛들은 같은 활성 함수를 가진다.
- 최종 출력 레이어의 경우 문제유형에 따른 표준 활성화 함수가 존재한다.
- 은닉층(hidden layer)의 경우 **ReLU** 함수를 주로 사용

### 주요 활성함수(Activation function)
#### 6-1. **Sigmoid(logistic function)**(주로 이진 분류를 위한 네트워크의 출력층의 활성함수로 사용)
- ![image](https://user-images.githubusercontent.com/77317312/115038770-cf386100-9f0a-11eb-966a-77f6def474b8.png)
- 한계
  - 초기 딥러닝 모델의 활성함수로 많이 사용되었으나 레이어가 깊어지면 기울기 손실 문제를 발생시키는 문제가 있다.
  - 함수값의 중심이 0이 아니어서 학습이 느려지는 단점이 있다.
    - X의 값이 0일 때 0.5를 반환
- **Binary classification(이진 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 사용된다.**
  - 위와 같은 한계때문에 hidden layer(은닉층)의 활성함수로는 잘 사용되지 않는다.
>   #### 기울기 소실(Gradient Vanishing)
>   - 최적화 과정에서 gradient가 0과 밑단층(Bottom Layer)의 가중치들이 학습히 안되는 
------------
#### 6-2. **Hypterbolic tangent**
- ![image](https://user-images.githubusercontent.com/77317312/115215232-4570d980-a13e-11eb-8572-9a28e07d528f.png)
- ![image](https://user-images.githubusercontent.com/77317312/115215301-57eb1300-a13e-11eb-8daf-666f7f833b24.png)
- Output이 0을 중심으로 분포하므로 sigmoid보다 학습에 효율적이다.
- 여전히 기울기 소실(gradient Vanishing) 문제를 발생시킨다.
- 주로 output으로 많이 사용된다. -> 이런 경우가 많지 않음
------------
#### 6-3. **ReLU(Rectified Linear Unit)**
- ![image](https://user-images.githubusercontent.com/77317312/115215600-a13b6280-a13e-11eb-8aa7-7d8d1fe4648c.png)
- 기울기 소실(Gradient Vanishing) 문제를 어느 정도 해결
- 0이하의 값(z <= 0)들에 대해 뉴런이 죽는 단점이 있다.
- **leaky ReLU**
- ![image](https://user-images.githubusercontent.com/77317312/115215854-e069b380-a13e-11eb-8573-3b78ea8a4c34.png)
- ReLU의 Dying ReLU 현상을 해결하기 위해 나온 함수 - 음수 z를 0으로 반환하지 않고 alpha(0 ~ 1 사이 실수)를 곱해 반환한다.
------------
#### 6-4. Softmax (주로 다중 분류를 위한 네트워크의 출력층의 활성함수로 사용)
- ![image](https://user-images.githubusercontent.com/77317312/115216081-2161c800-a13f-11eb-8faf-5e6e2ce5ff41.png)
- **Multi-class classification(다중 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 사용된다.**
  - 은닉층의 활성함수로 사용하지 않는다.
- 각 class의 score를 정규화 하여 각 class에 대한 확률값으로 변환
  - 출력노드들의 값은 0 ~ 1 사이의 실수로 변환되고 그 값의 총합은 1이 된다.
- ![image](https://user-images.githubusercontent.com/77317312/115217185-3559f980-a140-11eb-8970-0225eeb6f853.png)

## 7. Optimizer(최적화 방법)
- loss function을 기반으로 네트워크가 어떻게 업데이트 될지를 결정하는 알고리즘
  - 경사하강법과 오차 역전파(back propagation) 알고리즘을 이용해 weight를 최적화한다.

### 7-1. Gradient Decent(경사하강법)
- **최적화**
  - train 시 손실 값을 줄이기 위해 파라미터(weight, bias)를 update 과정
- **Gradient Decent(경사하강법)**
  - 최적화를 위해 파라미터들에 대한 Loss function의 Gradient값을 구해 Gradient의 반대 방향으로 일정크기 만큼 파라미터들을 업데이트 하는 것
- ![image](https://user-images.githubusercontent.com/77317312/115217844-dcd72c00-a140-11eb-9635-7bd3c55f8475.png)
#### 파라미터 업데이트 단위
- **Batch Gradient Decent (배치 경사하강법)**
  - loss를 계산할 때 전체 학습데이터를 사용해 그 평균값을 기반으로 파라미터를 최적화한다.
  - 많은 계산량이 필요해서 **속도가 느리다.** 학습 데이터가 클 경우 **메모리가 부족**할 수 있다.
- **Mini Batch Stochastic Gradient Decent(미니배치 확률적 경사하강법)**
  - loss 계산 시 지정한 데이터 양(batch size)만큼 마다 계산해 파라미터를 업데이트 한다.
  - 계산은 빠른 장점이 있지만 최적값을 찾아 가는 방향이 불안정하여 부정확 하다.
  - 그러나 반복 횟수를 늘리면 Batch방식과 유사한 결과로 수렴한다.
  - ![image](https://user-images.githubusercontent.com/77317312/115218548-a0f09680-a141-11eb-8a3f-2d23fdfcd5a9.png)
----------
#### 오차 역전파(Back propagation)
- 학습 시 파라미터 최적화할 때 추론한 역방향으로 loss를 전달하여 단계적으로 파라미터를 업데이트하는 것
  - loss에서부터(뒤에서 부터) 한단계식 미분해 gradient값을 구하고 이를 Chain rule(연쇄법칙)에 의해 곱해가면서 파라미터를 최적화

- **계산 그래프(computational graph)**
  - 복잡한 계산 과정을 자료구조의 하나인 그래프로 표현한 것
  - 그래프는 노드와 엣지로 구성됨
  - **계산 그래프 예**
  - ![image](https://user-images.githubusercontent.com/77317312/115219185-3855e980-a142-11eb-8d86-b0647374b4db.png)
- **계산 그래프 장점**
  - 계산 그래프를 사용한 문제 풀이 절차
    - 순전파, 역전파
  - 특징/장점
    - **국소적 계산**을 통해 결과를 얻는다.
      - 각 노드의 계산은 자신과 관계된 정보만 가지고 계산한 뒤 그 결과를 다음으로 출력
    - 복잡한 계산을 단계적으로 나눠 처리하므로 문제를 단순하게 만들어 계산할 수 있다.
    - **딥러닝에서 역전파를 이용해 각 가중치 업데이트를 위한 미분 계산을 효육적으로 할 수 있다.**

### 7-2. SGD를 기반으로 한 주요 옵티마이저
- 방향성을 개선한 최적화 방법
  - **Momentum**
  - NAG(Nesterov Accelerated Gradient)
- 학습률을 개선한 최적화 방법
  - Adograd
  - **RMSProp**
- 방향성 + 학습률 개선 최적화 방법
  - **Adam** - 제일 많이 쓰임
- 웬만하면 Adam 사용하자
- ![image](https://user-images.githubusercontent.com/77317312/115220176-350f2d80-a143-11eb-967d-8ec964d91426.png)
- https://www.slideshare.net/yongho/ss-79607172
- https://www.tensorflow.org/api_docs/python/tf/keras/activations
