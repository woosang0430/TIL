# DNN(Deep Neural Network)
## 신경망 구성요소
- 딥러닝 프로세스
- ![image](https://user-images.githubusercontent.com/77317312/115013468-0c412b00-9eec-11eb-8473-da4fc65fd702.png)
- **층(Layer)** : Network를 구성하는 layer(층)
- **손실함수(loss function)** : 가중치를 어떻게 업데이트할 지 예측 결과와 Ground truth(실제타깃) 사이의 차이를 정의
- **optimizer** : 가중치를 업데이트하여 모델의 성능을 최적화

### 1. 유닛/노드/뉴런 (unit, node, neuron)
- Tensor를 입력받아 tensor를 출력하는 데이터 처리 모듈
  - input -> output
- 입력 값에 Weight(가중치)를 곱하고 bias(편향)을 더한 뒤 활성화 함수를 거쳐 출력한다.
- 하나의 노드 구성
- ![image](https://user-images.githubusercontent.com/77317312/115013922-a4d7ab00-9eec-11eb-86ba-cf34c3dff108.png)
  - input vector(입력값) : X = (x1, x2, x3)^T
  - Weights(가중치) : W = (w1, w2, w3)^T
  - Bias(편향) : b
  - ACtivation function(활성함수)
    - 활성함수로는 비선형 함수를 사용
    - ![image](https://user-images.githubusercontent.com/77317312/115014302-1dd70280-9eed-11eb-807f-2a95c0a614a9.png)

### 2. 레이어/층(Layer)
- **input layer(입력층)**
- **output Layer(출력층)** 
- **Hidden Layer(은닉층)**
- **Network(망)** : layer들의 연결
- 많이 사용되는 Layer의 예
  - **Fully connected layer(Dense layer)** : 거의 추론 단계에서 사용
  - **Convolution layer** : 이미지쪽에서 많이 사용
  - **Recurrent layer** : 시계열 연속된 데이터
  - **Embedding layer** : 텍스트를 다룰 떄
- Layers : https://www.tensorflow.org/api_docs/python/tf/keras/layers

### 3. 모델
- Layer를 쌓아 만드는 네트워크
- 이전 레이어의 출력을 input으로 받아 output을 주는 층을 순서대로 쌓음
- 적절한 network 구조(architecture)를 찾는 것은 과학 보다는 예술의 경지! 많은 경험이 필요
- 기존의 잘 작동한 구조를 기반으로 구현하는 방식으로 접근
- ![image](https://user-images.githubusercontent.com/77317312/115015222-4c091200-9eee-11eb-8c3d-82b3f58caa09.png)

### .번외 딥러닝(Deep learning)
- 신경망이 많아지면 깊은 딥러닝이라고 한다.
- ![image](https://user-images.githubusercontent.com/77317312/115015354-7955c000-9eee-11eb-8f05-a1541698a09e.png)

### 4. 손실함수(loss function, 비용함수)
- Model을 통해 나온 예측값(prediction) y^와 실제 데이터(output) y의 차이를 수령화하는 함수
- 훈련하는 동안 최소화될 값으로 이 값을 바탕으로 파라미터(가중치와 편향)을 업데이트 한다.
- 문제의 종류에 따라 다른 loss함수를 사용
#### 4-1. 해결하려는 문제의 종류에 따라 표준적인 loss function이 존재함
- **Binary classification(이진 분류)**
  - binary_crossentropy를 loss function으로 사용
  - ![image](https://user-images.githubusercontent.com/77317312/115037462-73b9a380-9f09-11eb-84de-f0c4891af31e.png)

- **Multi-class classification(다중 클래스 분류)**
  - categorical_crossentropy를 loss function으로 사용
  - ![image](https://user-images.githubusercontent.com/77317312/115037721-bbd8c600-9f09-11eb-880f-4dd37fb2845e.png)

- **Regression(회귀)**
  - Mean squared error를 loss function으로 사용('mse'로 지정)
  - ![image](https://user-images.githubusercontent.com/77317312/115037881-eb87ce00-9f09-11eb-9054-ee0f286d7fe0.png)
- https://www.tensorflow.org/api_docs/python/tf/keras/losses

### 5. 평가지표(metrics)
- 모델의 성능을 평가하는 지표
- 손실함수와 차이
  - 손실함수는 모델을 학습할 때 가중치 업데이트를 위한 오차를 구할 때 사용
  - 평가지표 함수는 모델의 성능을 확인하는데 사용

### 6. 활성 함수(activation function)
- 각 유닛이 입력결과를 처리한 후 출력하기 위해 거치는 함수
- 같은 층(layer)의 모든 유닛들은 같은 활성 함수를 가진다.
- 최종 출력 레이어의 경우 문제유형에 따른 표준 활성화 함수가 존재한다.
- 은닉층(hidden layer)의 경우 **ReLU** 함수를 주로 사용

#### 6-1. 주요 활성함수(Activation function)
- **Sigmoid(logistic function)**
- ![image](https://user-images.githubusercontent.com/77317312/115038770-cf386100-9f0a-11eb-966a-77f6def474b8.png)
- 한계
  - 초기 딥러닝 모델의 활성함수로 많이 사용되었으나 레이어가 깊어지면 기울기 손실 문제를 발생시키는 문제가 있다.
  - 함수값의 중심이 0이 아니어서 학습이 느려지는 단점이 있다.
    - X의 값이 0일 때 0.5를 반환















