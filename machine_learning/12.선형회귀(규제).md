# 선형회귀 개요
- 선형 회귀는 종족 변수 y와 한 개 이상의 독립 변수 X와의 선형 상관 관계를 모델링하는 회귀분석 기법

## 선형회귀 모델
- ![image](https://user-images.githubusercontent.com/77317312/113233341-fae80400-92d9-11eb-9283-689b6537ad3e.png)

## 손실(loss)함수/ 오차(error)함수/ 비용(cost)함수/ 목적(objective)함수
- 모델이 출력한 예측값과 실제 값 사이의 차이를 계산하는 함수
- 주로 학습할 때 사용. 파라미터가 맞는지 틀린지의 대한 차이 지표
- 평가함수와 손실함수는 다른 역할을 한다.
#### MSE
- 손실 지표로도 같이 사용

## 최적화(Optimize)
- 손실함수의 값이 최소화 되도록 모델을 학습하는 과정
- 최적화의 두가지 방법 --> W, b에 대한
  - 정규방정식(공식으로 똭!)
  > - ~~정규방정식이 없으면...경사하강법~~ ㄱㄱ..
  - 경사하강법(일일이 다 넣어 확인 -> 오차가 작아지는 방법으로 넣어줘야함)

### Boston DataSet
> - `CRIM`	: 지역별 범죄 발생률
> - `ZN`	: 25,000 평방피트를 초과하는 거주지역의 비율
> - `INDUS`: 비상업지역 토지의 비율
> - `CHAS`	: 찰스강에 대한 더미변수(강의 경계에 위치한 경우는 1, 아니면 0)
> - `NOX`	: 일산화질소 농도
> - `RM`	: 주택 1가구당 평균 방의 개수
> - `AGE`	: 1940년 이전에 건축된 소유주택의 비율
> - `DIS`	: 5개의 보스턴 고용센터까지의 접근성 지수
> - `RAD`	: 고속도로까지의 접근성 지수
> - `TAX`	: 10,000 달러 당 재산세율
> - `PTRATIO` : 지역별 교사 한명당 학생 비율
> - `B`	: 지역의 흑인 거주 비율
> - `LSTAT` : 하위계층의 비율(%)
> - `MEDV`	: Target.  본인 소유의 주택가격(중앙값) (단위: $1,000)
```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_boston

boston = load_boston()
X, y = boston['data'], boston['target']

df = pd.DataFrame(X, columns=boston.feature_names)
df['MEDV'] = y
```
## 전처리
- 선형회귀 모델 사용시 전처리
  - 범주형 : One Hot Encoding
  - Feature Scaling을 통해서 각 컬럼들의 값의 단위를 맞춰준다.
    - StandardScaler를 사용해 scaling하는 경우 성능이 더 잘나온다.
```python
chas_df = pd.get_dummies(df['CHAS'])
chas_df.columns = ['CHAS_0', 'CHAS_1']

boston_df = df.join([chas_df])
boston_df = boston_df.drop(columns='CHAS')

y = boston_df['MEDV']
X = boston_df.drop(columns='MEDV')

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)
train_columns = X_train.columns

# 선형 회귀 모델은 scale 영향을 많이 받는다.
# OneHotEncoing한 뒤 train/test 분리 후 인코딩. (0, 1 이 다른 값으로 바뀌지만 일관되게 바뀌어 모델에 영향을 주지 않는다.)


from sklearn.preprocessing import StandardScaler, MinMaxScaler
# scaler = MinMaxScaler()
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```
## LinearRegression
- 가장 기본적인 선형 회귀 모델
```python
# 평가 지표 출력 함수 정의
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def print_metrics(y, pred, title=None):
    mse = np.round(mean_squared_error(y, pred), 3)
    rmse = np.round(np.sqrt(mse), 3)
    mae = np.round(mean_absolute_error(y, pred),  3)
    r2 = np.round(r2_score(y, pred), 3)
    if title:
        print(title)
    print(f'MSE:{mse}, RMSE:{rmse}, MAE:{mae}, R2:{r2}')

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

# 가중치(회귀계수)와 절편

print('가중치')
lr.coef_ # 학습한 가중치
# >> 가중치
# >> array([-1.16449629,  1.49915257, -0.05268964, -1.91400079,  2.08637507,
# >>         0.45488752, -3.15772431,  2.71071007, -2.07814318, -1.7549281 ,
# >>         1.08897073, -4.10237313, -0.24654802,  0.24654802])

print('절편')
lr.intercept_
# >> 절편
# >> 21.84183168316832

# 예측
pred_train = lr.predict(X_train_scaled)
pred_test = lr.predict(X_test_scaled)

print_metrics(y_train, pred_train, title='LinearRegressor: Train')
print_metrics(y_test, pred_test, title="LinearRegressor: Trest")
# >> LinearRegressor: Train
# >> MSE:19.231, RMSE:4.385,MAE:3.083, R2:0.75
# >> LinearRegressor: Trest
# >> MSE:34.414, RMSE:5.866,MAE:4.061, R2:0.671
```
## 실제 값과 예측 가격 시각화
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

# 한글 및 음수 표시 설정
mpl.rcParams['font.family'] = 'malgun gothic'
mpl.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(15, 5))
plt.plot(range(len(y_test)), y_test, label='실제값', marker='x')
plt.plot(range(len(pred_test)), pred_test, label='예측값', marker='o')

plt.legend()
plt.show()
```
- ![image](https://user-images.githubusercontent.com/77317312/113246325-c8e39b80-92f3-11eb-9b6a-f8e205275700.png)
## 다항회귀(polynomial Regression)
- 단순한 직선형 보다 복잡한 비선형의 데이터셋을 학습하기 위한 방식
- Feature들을 거듭제곱한 것과 Feature들을 곱한 새로운 특성들을 추가한 뒤 선형모델로 훈련시킨다.
- `PolynomialFeatures` Transfomer를 사용

## 토이 데이터 예제
```python
import numpy as np
import matplotlib.pyplot as plt

m = 100
X = 6 * np.random.rand(m, 1) - 3 # shape(100, 1) 0 ~ 1 실수
y = X**2 + X + 2 + np.random.normal(0, 1, size=(m, 1))

plt.figure(figsize=(6,5))
plt.scatter(X, y, alpha=0.7)
plt.show()
```
- ![image](https://user-images.githubusercontent.com/77317312/113265111-b4f96300-930e-11eb-82b6-cf946b3ac440.png)
- 선형회귀로만으로 정확도를 높일 수 없으므로 다항회귀 사용
```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(X, y)
lr.coef_, lr.intercept_
# >> (array([[0.80688141]]), array([5.03010368]))

from sklearn.metrics import mean_squared_error, r2_score

pred = lr.predict(X)
mean_squared_error(y, pred), r2_score(y, pred)
# >> (7.521143122544466, 0.20594133771687428)

X_new = np.linspace(-3, 3, 100).reshape(-1, 1)
pred_new = lr.predict(X_new)

plt.figure(figsize=(6,5))

plt.scatter(X, y, alpha=0.7)
plt.plot(X_new, pred_new, color='red')

plt.grid(True)
plt.show()
```
- ![image](https://user-images.githubusercontent.com/77317312/113265851-90ea5180-930f-11eb-9c5b-d17f20db3f2d.png)
- 위의 결과값 처럼 선형회귀로는 예측이 어렵다
## X의 Feature를 늘려서 다항식이 되도록 처리
```python
from sklearn.preprocessing import PolynomialFeatures

# degree : 최고차항
poly_f = PolynomialFeatures(degree=2, include_bias=False)
# include_bias : 상수항 추가 여부(default : True)

X_poly = poly_f.fit_transform(X)

# 생성된 컬럼들이 어떻게 생성되었는지 확인 - 컬럼의 으름
poly_f.get_feature_names()
# >> ['x0', 'x0^2']

lr2 = LinearRegression()
lr2.fit(X_poly, y)

lr2.coef_, lr2.intercept_ # W, b
# >> (array([[1.10699305, 1.03609309]]), array([1.94717129]))

pred2 = lr2.predict(X_poly)
mean_squared_error(y, pred2), r2_score(y, pred2)
# >> (0.9050468274177637, 0.9044479992770291)

X_new_poly = poly_f.transform(X_new)
y_new2 = lr2.predict(X_new_poly)

plt.figure(figsize=(6,5))

plt.scatter(X, y)
plt.plot(X_new, y_new2, color='red')

plt.grid(True)
plt.show()
```
- ![image](https://user-images.githubusercontent.com/77317312/113266935-c2175180-9310-11eb-9df0-7e9251c39608.png)
## 다항회귀 boston dataset 적용
```python
# 전처리
poly_f = polynomialFeature(degree=2, include_bias=False)
X_train_scaled_poly = poly_f.fit_transform(X_train_scaled)
X_test_scaled_poly = poly_f.transform(X_test_scaled)

# 학습
lr = LinearRegression()
lr.fit(X_train_scaled_poly, y_train)

# 검증
pred_train2 = lr.predict(X_train_scaled_poly)
pred_test2 = lr.predict(X_test_scaled_poly)

print('PolynomialFeature 적용전 결과')
print_metrics(y_train, pred_train, title='train')
print_metrics(y_test, pred_test, title='test')
# >> PolynomialFeature 적용전 결과
# >> train
# >> MSE:19.854, RMSE:4.456,MAE:3.039, R2:0.742
# >> test
# >> MSE:34.342, RMSE:5.86,MAE:4.025, R2:0.672

print('PolynomialFeature 적용 후 결과')
print_metrics(y_train, pred_train2, title='train')
print_metrics(y_test, pred_test2, title='test')
# >> PolynomialFeature 적용후 결과
# >> train
# >> MSE:5.239, RMSE:2.289,MAE:1.673, R2:0.932
# >> test
# >> MSE:13.073, RMSE:3.616,MAE:2.818, R2:0.875
```
-- 오차가 확실히 줄어든 것을 확인 할 수 있다.
# 규제 (Regularization)
- 선형 회귀 모델에서 과적합 문제를 해결하기 위해 가중치(회귀계수)에 페널티 값을 적용하는 것.
- 입력데이터의 Feature들이 너무 많은 경우 과적합이 발생
  - Feature수에 비해 관측치 수가 적은 경우 모델이 복잡해 지면서 과정합이 발생
- 해결
  - 데이터 더 수집
  - Feature selection
    - 불필요한 Features들 제거
  - **규제(Regularization)** 를 통해 Feature들에 곱해지는 가중치가 커지지 않도록 제한
# 1. Ridge Regression
- ![image](https://user-images.githubusercontent.com/77317312/113269529-8336cb00-9313-11eb-8bdc-7afcd991c082.png)
- **a[alpha]** 는 **하이퍼파라미터** 로 모델을  얼마나 많이 규제할지 조절
  - a = 0 에 가까울수록 규제가 약해진다.(0일 경우 선형회귀동일)
  - a가 커질수록 모든 가중치가 작아져 입력데이터의 Feature들 중 중요하지 않은 Feature의 output에 대한 영향력이 작아진다.
- `from sklearn.linear_model import Ridge`

# 2. Lasso(Least Abosolut Shrinkage and selection Operator) Regression
- ![image](https://user-images.githubusercontent.com/77317312/113375871-5c2bd800-93ab-11eb-8fc2-46b8f07d538d.png)
- Lasso  회귀의 상대적으로 덜 중요한 특성의 가중치를 0으로 만들어 자동으로 Feature Selection이 된다.
- `from sklearn.linear_model import Lassco`

### 둘의 문법은 똑같지만 Lasso의 경우 덜 중요한 가중치를 0으로 만든다.
```python
from sklearn.linear_model import Ridge

ridge_1 = Ridge() #alpha : 기본값 -> 1
# ridge_1 = Ridge(alpha=25)
ridge_1.fit(X_train_scaled, y_train)

pred_train = ridge_1.predict(X_train_scaled)
pred_test = ridfe_1.predict(X_test_scaled)

print_metrics(y_train, pred_train, title='Ridge alpha 1: train')
print_metrics(y_test, pred_test, title='Ridge alpha 1: test')
# >> Ridge alpha 1: Train
# >> MSE:19.233, RMSE:4.386,MAE:3.079, R2:0.75
# >> Ridge alpha 1: Trest
# >> MSE:34.392, RMSE:5.864,MAE:4.058, R2:0.671
```
## GridSearchCV
```python
from sklearn.model_selection import GridSearchCV

param = dict(alpha=[0.01,0.1,0.5,1,10,20,30,40,50,100])
ridge = Ridge()
gs_ridge = GridSearchCV(ridge,
                        param_grid=param,
                        cv=5,
                        n_jobs=-1,
                        scoring=['r2', 'neg_mean_squared_error'],
                        refit='r2')
# 학습
gs_ridge.fit(X_train_scaled, y_train)

# 가중치 확인
import pandas as pd
cv_result = pd.DataFrame(gs_ridge.cv_results_).sort_values('rank_test_neg_mean_squared_error')
cv_result.filter(like='mean_test').join(cv_result.filter(like='param'))
```
- output

|   |	mean_test_r2 | mean_test_neg_mean_squared_error | param_alpha	| params |
| -- | -- | -- | -- | -- |
| 3	| 0.718794	| -21.376445	| 1	| {'alpha': 1} |
| 2	| 0.718655	| -21.385044	| 0.5	| {'alpha': 0.5} |
| 1	| 0.718526	| -21.393176	| 0.1	| {'alpha': 0.1} |
| 4	| 0.718805	| -21.394979	| 10 |	{'alpha': 10} |
|...|...|...|...|...|
```python
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
# g = GradientBoostingRegressor(n_estimators=100, max_depth=3)
g = XGBRegressor(n_estimators=300, max_depth=1)

# 학습
g.fit(X_train_scaled, y_train)

# 검증
print(mean_squared_error(y_train, g.predict(X_train_scaled)))
print(mean_squared_error(y_test, g.predict(X_test_scaled)))
# >> 4.620336900102862
# >> 26.718328950405017

pd.Series(g.feature_importances_, index=X_train.columns).sort_values(ascending=False)
# >> LSTAT      0.470713
# >> RM         0.210982
# >> PTRATIO    0.075741
# >> NOX        0.049113
# >> ...
```
# 3. ElasticNet(엘라스틱넷)
- ![image](https://user-images.githubusercontent.com/77317312/113376074-e1af8800-93ab-11eb-8d30-dd0bc4cbe6e8.png)
- 릿지와 라쏘를 절충한 모델
- 규제항에 릿지, 라쏘 규제항을 더해서 추가
- 혼합비율 *r*을 사용해 혼합정도를 조절
- *r* = 0이면 릿지와 같고 *r* = 1이면 라쏘와 같다.
```python
from sklearn.linear_model import ElasticNet

elastic = ElasticNet(alpha=0.1, l1_ratio=0.6)
# alpha : 패널티, l1_ratio : 비율(Lasso쪽. 위의것으로 보면 앞의것의 r 지정.)
elastic.fit(X_train_scaled, y_train)

pred_train = elastic.predict(X_train_scaled)
pred_test = elastic.predict(X_test_scaled)

print_metrics(y_train, pred_train, title='ElasticNet alpha 0.1: Train')
print_metrics(y_test, pred_test, title="ElasticNet alpha 0.1: Trest")
# >> ElasticNet alpha 0.1: Train
# >> MSE:19.854, RMSE:4.456,MAE:3.039, R2:0.742
# >> ElasticNet alpha 0.1: Trest
# >> MSE:34.342, RMSE:5.86,MAE:4.025, R2:0.672
```
## 규제 정리
- 일반적으로 선형회귀의 경유 어느정도 규제가 있으면 성능이 좋다.
- 기본적으로 ridge를 사용한다.
- target에 영향을 주는 Feature가 몇 개 뿐일 경우 특성의 가중치를 0으로 만들어 주는 Lasso 사용
- 특성 수가 학습 샘플 수 보다 많거나 Feature간에 연관성이 높을 때는 elasticnet을 사용
