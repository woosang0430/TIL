## 1. 범주형 데이터 전처리
- 사이킷런은 숫자형만 처리할 수 있다.
- 범주형 Feature 처리
  - Label Encoding
  - One Hot Encoding
### 범주형 변수(Categorical Variable)
- 어떤 분류에 대한 속성을 가지는 변수
  - ex) 성별 - 남/녀, 혈액형 - A, B, AB, O, 성적 - A, B, C, D, F
- 비서열(Unordered) 변수
  - 범주에 속한 값간에 **서열(순위)가 없는 변수**
  - 성별, 혈액형
- 서열(Ordered) 변수
  - 범주에 속한 값 간에 **서열(순위)가 있는 변수**
  - 성적, 직급
- 사이킷런은 문자열 값을 숫자 형으로 변환해야 한다.
  - 범주형 변수 => **정수값**으로 변환
  - 단순 문자열(범주형X) => 일반적으로 **제거**

## 1-1. 레이블 인코딩(Label Encoding)
- 문자열(범주형) 값을 오름차순 정렬 후 0부터 1씩 증가하는 값으로 변환
- 숫자의 차이가 모델에 영향을 주지 않는 트리 계열 모델(**의사결정나무, 랜덤포레스트**)에 적용
- ![image](https://user-images.githubusercontent.com/77317312/111961838-e155f800-8b34-11eb-8067-cde647b99db6.png)
- `sklearn.preprocessing.LabelEncoder` 사용
  - `fit()` : 어떻게 변환할 지 학습
  - `transform()` : 문자열을 숫자로 변환
  - `fit_transform()` : 학습과 변환을 한번에 처리
  - `inverse_transform()` : 숫자를 문자열로 변환
  - `classes_` : 인코딩한 클래스 조회
### Label_Encoding 실습
- adult data에 label encoding 적용
- 데이터셋은 1994년 인구조사 데이터 베이스에서 추출한 미국 성인의 소득 데이터셋
- target은 income이며 수입이 $50,000 이하인지 초과인지로 분류되어 있음
- https://archive.ics.uci.edu/ml/datasets/adult
```python
import pandas as pd

# 데이터 불러와서 정리
cols = ['age', 'workclass','fnlwgt','education', 'education-num', 'marital-status', 'occupation','relationship', 'race', 'gender','capital-gain','capital-loss', 'hours-per-week','native-country', 'income']
adult = pd.read_csv('data/adult.data',
                    header=None, # 컬럼명 지정 안함
                    names=cols,  # 직접 컬럼명 지정
                    skipinitialspace=True, # 데이터의 앞뒤 공백 제거
                    na_values='?') # nan이 아닌 값 nan처리
adult.dropna(inplace=True) # 결측치 처리

encoding_columns = ['workclass','education','marital-status', 'occupation','relationship','race','gender','native-country', 'income']
not_encoding_columns = ['age','fnlwgt', 'education-num','capital-gain','capital-loss','hours-per-week']
                    
from sklearn.preprocessing import LabelEncoder

# 각  컬럼(feature)마다 변환 전의 고유값들을 저장할 딕셔너리
enc_dict = {}

# apply에 넣어줄 함수 정의
def encoding_label(x):
  # 범주형 Series를 매개변수로 받아 LabelEncoding 처리 후 반환
  le = LabelEncoder()
  ret = le.fit_transform(x)
  enc_dict[x.name] = le.classes_ # x.name : 컬럼명 조회
  return ret

adult_enc_df = adult[encoding_columns].apply(encoding_label)
adult_enc_df
```
- ![image](https://user-images.githubusercontent.com/77317312/111968449-80322280-8b3c-11eb-9946-8fcaeb3646b2.png)
```python
# 변환한 값의 고유값 알아보기
enc_dict['workclass'][5], enc_dict['occupation'][3]
# ('State-gov', 'Exec-managerial')

# 변환하지 않은 컬럼 붙여주기
adult_df = pd.concat([adult_enc_df, adult[not_encoding_columns]], axis=1)
```
### 학습
```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# 데이터 셋 분리
# 1. income(y) 분리
y = adult_df['income']
X = adult_df.drop(colums='income')

# 2. train, validation, test set으로 분리
# test set 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)

X_train, X_val, y_train,  y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y, random_state=1)

# 3. 모델 생성 및 학습
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(max_depth=7, random_state=1)
tree.fit(X_train, y_train)

# 4. 검증
from sklearn.metrics import accuracy_score

pred_train = tree.predict(X_train)
pred_val = tree.predict(X_val)

print('train 정확도 :', accuracy_score(y_train, pred_train))
print('val 정확도 :', accuracy_score(y_val, pred_val))
# train 정확도 : 0.8585875331564987
# val 정확도 : 0.8509862423338306

# test set으로 최종 평가
pred_test = tree.predict(X_test)
print('test accuracy :', accuracy_score(y_test, pred_test))
# test accuracy : 0.8508204873197415
```
## 1-2. 원핫 인코딩(One-Hot Encoding)
- n개의 클래스를 n 차원의 One-Hot 벡터로 표현되도록 변환
  - 고유값들을 feature로 만들고 정답에 해당하는 열은 1, 나머지는 0으로 표시
- 숫자의 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에서 범주형 데이터 변환시 사용
- ![image](https://user-images.githubusercontent.com/77317312/111986280-16704380-8b51-11eb-839a-7bb8a5eb0185.png)
- **사이킷런**
  - `sklearn.preprocessing.OneHotEncoder` 이용
    - `fit()` : 어떻게 변환할 지 학습
    - `transform()` : 문자열를 숫자로 변환
    - `fit_transform()` : 학습과 변환 한번에 처리
    - `get_feature_names()` : 원핫인코딩으로 변환된 컬럼의 이름 반환
  - DataFrame을 넣을 경우 모든 변수들을 변환
    - 범주형 컬럼만 처리하도록 해야한다.
- **Pandas**
  - `pandas.get_dummies(DataFrame [, columns=[변환할 컬럼명]])` 함수 이용
  - DataFrame에서 범주형(문자열) 변수만 변환한다.
### One-Hot Encoding 실습
```python
import pandas as pd
cols = ['age', 'workclass','fnlwgt','education', 'education-num', 'marital-status', 'occupation','relationship', 'race', 'gender','capital-gain','capital-loss', 'hours-per-week','native-country', 'income']

# 데이터 생성
data = pd.read_csv('data/adult.data',
                    header=None,
                    names=cols,
                    skipinitialspace=True,
                    na_values='?')
# 결측치 제거
data.droupna(inplace=True)
```
> - 범주형: 'workclass','education', 'education-num', 'marital-status', 'occupation','relationship', 'race', 'gender', 'hours-per-week','native-country', 'income'
> - 연속형: 'age', fnlwgt', 'capital-gain', 'capital-loss'
> - **위의 컬럼들중 'workclass','education', 'occupation', 'gender', 'hours-per-week', 'income' 만 사용한다.**
> - **income 은 Label Encoding으로 처리한다.**
>    - output 데이터
```python
categorical_cols = ['workclass','education', 'occupation', 'gender', 'hours-per-week', 'income']
adult_df = data[categorical_cols]
   
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# income 컬럼은 LabelEncoding
le = LabelEncoder()
y = le.fit_transform(adult_df['income'])

# 나머지 컬럼 OneHotEncoding
ohe = OneHotEncoder(sparse=False)
X = ohe.fit_transform(adult_df[adult_df.columns[:-2]])

# 데이터 프레임으로 변환
X_df = pd.DataFrame(X, columns=ohe.get_feature_names())
X_df['hours-per-week'] = adult_df['hours-per-week'].values
## 그냥 붙이게 되면 인덱스 값이 다르게 때문에 nan값이 발생  주의하자
```
### 판다스 pd.get_dummies() 사용
```python
pd.get_dummies(adult_df.drop(columns='income'))
# 한줄로 끝...
```
### 데이터 학습
```python
from sklearn.preprocessing import train_test_split

# train, test set 분리
X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.3, stratify=y, random_state=1)

# 모델 생성 및 학습
from sklearn.linear_model import LogisticRegression # 선형모델
from sklearn.metrics import accuracy_score

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

## 검증
pred_train = lr.predict(X_train)
pred_test = lr.predict(X_test)
print('train accuracy :', accuracy_score(y_train, pred_train))
print('test accuracy :', accuracy_score(y_test, pred_test))
# train accuracy : 0.9835680751173709
# test accuracy : 0.972027972027972
```
## 2. 연속형(수치형) 데이터 전처리
### Feature Scaling(정규화)
- 각 feature가 가지는 값들의 숫자 범주(Scale)가 다를 경우 값의 범위를 일정한 범위로 맞추는 작업
- 트리계열을 제외한 대부분의 머신러닝 알고리즘들이 feature의 스케일에 영향을 받는다.
  - 선형모델, SVM모델, 신경망 모델 등
- **Scaling(정규화)** 은 train set으로 fitting 한다. test set이나 예측할 새로운 데이터는 train set으로 fitting한 것으로 변환한다.
- 종류
  - `Standardization`
  - `Min Max Scaling`
- 함수
  - `fit()` : 어떻게 변환할 지 학습
  - `transform()` : 변환
  - `fit_transform()` : 학습과 변환을 한번에 처리
## 2-1. StandardScaler
- feature의 값들이 표준정규분포에 따르도록 변환(평균=0, 표준편차=1)
  - 0을 기준으로 데이터들이 모여있게 된다.
- 특히 `SVM`이나 `선형회귀`, `로지스틱 회귀` 알고리즘(선형모델)은 데이터셋이 표준정규분포를 따를때 성능이 좋다.
- ![image](https://user-images.githubusercontent.com/77317312/112090350-b9b76c00-8bd6-11eb-8dbf-0105a2f94e6e.png)
- `sklearn.preprocessing.StandardScaler`를 이용
```python
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

iris = load_iris()
X, y = iris['data'], iris['target']
iris_df = pd.DataFrame(X, columns=iris['feature_names'])

iris_df.head()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112115625-c058da00-8bfc-11eb-89ef-03ba90d1d90a.png)

```python
# 정규화
scaler = StandardScaler()
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)
iris_scaled_df = pd.DataFrame(iris_scaled, columns=iris['feature_names'])

iris_scaled_df.head()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112114680-af5b9900-8bfb-11eb-9661-57af944fcb0a.png)

## 2-2. MinMaxScaler
- 데이터셋의 모든 값을 0과 1 사이의 값으로 변환
- ![image](https://user-images.githubusercontent.com/77317312/112114801-cdc19480-8bfb-11eb-9ed8-fd8c1c61b6d1.png)
```python
from sklearn.preprocessing import MinMaxScaler
mm_scaler = MinMaxScaler()

mm_scaler.fir(iris_df)
iris_mm_scaled = mm_scaler.transform(iris_df)
iris_mm_scaled = pd.DataFrame(iris_mm_scaled, columns=iris['feature_names'])

iris_mm_scaled.head()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112115275-59d3bc00-8bfc-11eb-891a-5932f7a8e310.png)
## StandardScaler or MinMaxScaler
- 보통 StandardScaler를 자주 사용
- 하지만 두개 다 사용해보고 더 성능이 좋은걸 쓰자
- minmaxscaler는 새로운 데이터셋의 변환 과정에서 범위를 벗어나게된다.
  - 크게 상관은 없다고 하시지만 주의하자

## Scaling 실습(위스콘신 유방암 데이터셋)
- ID, 암측정값들, 진달결과 컬럼들로 구성
- scikit_learn에서 제공. load_breast_cancer() 함수 이용
```python
from sklearn.datasets import load_breast_cancer()
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

cancer = load_breast_cancer()
# input, output 값 분리
X, y = cancer['data'], cancer['target']

# 학습, 검증 데이터 분리(validation set을 분리하는게 원칙이나 데이터가 적어서 생략)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)

# scaling : train set으로 학습( 학습대상은 X-feature), train_set으로 학습한 scaler로 학습, 검증 데이터 변환
s_scaler = StandardScaler()
X_train_scaled = s_scaler.fit_transform(X_train)
X_test_scaled = s_scaler.transform(X_test)
```
#### scaling 안된 데이터
```python
# 모델 생성, 학습
svc = SVC(C=0.1, gamma=0.1)
# scaling 안된 데이터로 학습 및 예측
svc.fit(X_train, y_train)

# 검증
pred_train = svc.predict(X_train)
pred_test = svc.predict(X_test)

# 정확도
print(accuracy_score(y_train, pred_train)
print(accuracy_score(y_test, pred_test)
# >> 0.6267605633802817
# >> 0.6293706293706294
```
#### scaling한 데이터
```python
# 모델 생성, 학습
svc = SVC(C=0.1, gamma=0.1)

# scaling된 데이터로 학습 및 예측
svc.fit(X_train_scaled, y_train)

# 검증
pred_train = svc.predict(X_train_scaled)
pred_test = svc.predict(X_test_scaled)

# 정확도
print(accuracy_score(y_train, pred_train))
print(accuracy_score(y_test, pred_test))
# >> 0.9577464788732394
# >> 0.9090909090909091
```
