# Support Vector Machine (SVM)
## 선형(Linear) SVM
- 딥러닝 이전에 분류에서 뛰어난 성능으로 가장 활용도가 높았던 분류 모델
- 중간크기의 데이터셋과 특성(Feature)이 많은 복잡한 데이터셋에서 성능이 좋은 것으로 알려져있다.

## 목표 : support vector간의 가장 넓은 margin을 가지는 초평명(결정경계)를 찾는다.
> ### 초평면
> - 데이터가 존재하는 공간보다 1차원 낮은 부분 공간
>     - n차원의 초평면은 n-1 차원
>     - 공간을 나누기 위해 초평면을 사용
>     - 1차원 - 점, 2차원 - 선, 3차원 - 평면, 4차원이상 - 초평면
- Support Vector : 경계를 찾아내는데 기준이 되는 데이터포인트. 초평면(결정경계)에 가장 가까이 있는 vector(데이터포인트)를 말함
- margin : 두 support vector간의 너비
- margin이 넓은 결정경계를 만드는 함수를 찾는 것
- ![image](https://user-images.githubusercontent.com/77317312/112834966-9eb39300-90d3-11eb-8816-4240f7061ee7.png)

## Hard Margin, Soft Margin
- Overfitting을 방지하기 위해 어느정도의 오차를 허용하는 방식을 **Soft margin**
- 반대로 오차를 허용하지 않는 방식을 **Hard Margin**
- 노이즈가 있는 데이터나 선형적으로 분리 되지 않는 경우 **하이퍼파라미터인 C** 조정해 마진을 변경
- `C` : default=1
> - 파라미터값을 크게 주명 마진폭이 좁아져 마진 오류가 작아지지만 Overfitting이 일어날 가능성이 크다.
> - 파라미터값을 작게 주면 마진폭이 넓어져 마진 오류가 크다. 훈련데이터에서는 성능이 안좋아지나 **일반화**되어 테스트 **데이터의 성능**이 올라간다.
> - 하지만 underfitting이 날 가능성이 있다.

- underfitting : C값을 더 크게 잡는다.
- overfitting : C값을 더 작게 잡는다.
- - ![image](https://user-images.githubusercontent.com/77317312/112835614-77a99100-90d4-11eb-864c-ffcbd06f9811.png)
```python
import numpy as np
import pandas as pd

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)

# SVM은 선형모델이기 때문에 Scaling 작업이 필요
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# C가 커질수록 overfitting이 날 가능성이 높아진다.
# 작아질수록 underfitting이 날 가능성이 높아진다.
svc = SVC(kernel='linear',
          C=2.0,
          random_state=1)
svc.fit(X_train_scaled, y_train)

# 검증하기
pred_train = svc.predict(X_train_scaled)
pred_test = svc.predict(X_test_scaled)

accuracy_score(y_train, pred_train),  accuracy_score(y_test, pred_test)
```
