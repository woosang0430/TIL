# Support Vector Machine (SVM)
## 선형(Linear) SVM
- 딥러닝 이전에 분류에서 뛰어난 성능으로 가장 활용도가 높았던 분류 모델
- 중간크기의 데이터셋과 특성(Feature)이 많은 복잡한 데이터셋에서 성능이 좋은 것으로 알려져있다.

## 목표 : support vector간의 가장 넓은 margin을 가지는 초평명(결정경계)를 찾는다.
> ### 초평면
> - 데이터가 존재하는 공간보다 1차원 낮은 부분 공간
>     - n차원의 초평면은 n-1 차원
>     - 공간을 나누기 위해 초평면을 사용
>     - 1차원 - 점, 2차원 - 선, 3차원 - 평면, 4차원이상 - 초평면
- Support Vector : 경계를 찾아내는데 기준이 되는 데이터포인트. 초평면(결정경계)에 가장 가까이 있는 vector(데이터포인트)를 말함
- margin : 두 support vector간의 너비
- margin이 넓은 결정경계를 만드는 함수를 찾는 것
- ![image](https://user-images.githubusercontent.com/77317312/112834966-9eb39300-90d3-11eb-8816-4240f7061ee7.png)

## Hard Margin, Soft Margin
- Overfitting을 방지하기 위해 어느정도의 오차를 허용하는 방식을 **Soft margin**
- 반대로 오차를 허용하지 않는 방식을 **Hard Margin**
- 노이즈가 있는 데이터나 선형적으로 분리 되지 않는 경우 **하이퍼파라미터인 C** 조정해 마진을 변경
- `C` : default=1
> - 파라미터값을 크게 주명 마진폭이 좁아져 마진 오류가 작아지지만 Overfitting이 일어날 가능성이 크다.
> - 파라미터값을 작게 주면 마진폭이 넓어져 마진 오류가 크다. 훈련데이터에서는 성능이 안좋아지나 **일반화**되어 테스트 **데이터의 성능**이 올라간다.
> - 하지만 underfitting이 날 가능성이 있다.

- underfitting : C값을 더 크게 잡는다.
- overfitting : C값을 더 작게 잡는다.
- - ![image](https://user-images.githubusercontent.com/77317312/112835614-77a99100-90d4-11eb-864c-ffcbd06f9811.png)
```python
import numpy as np
import pandas as pd

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)

# SVM은 선형모델이기 때문에 Scaling 작업이 필요
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# C가 커질수록 overfitting이 날 가능성이 높아진다.
# 작아질수록 underfitting이 날 가능성이 높아진다.
svc = SVC(kernel='linear',
          C=2.0,
          random_state=1)
svc.fit(X_train_scaled, y_train)

# 검증하기
pred_train = svc.predict(X_train_scaled)
pred_test = svc.predict(X_test_scaled)

accuracy_score(y_train, pred_train),  accuracy_score(y_test, pred_test)
# >> (0.9929577464788732, 0.972027972027972)
```
## 커널 서포트 벡터 머신
### 비선형데이터 셋에 SVM 적용
- 선형으로 분리가 안되는 경우는??
- ![image](https://user-images.githubusercontent.com/77317312/112930879-15917000-9156-11eb-983a-d11343db64cd.png)
- 다항식 특성을 추가하여 차원을 늘려 선형 분리가 되도록 변환
- ![image](https://user-images.githubusercontent.com/77317312/112930905-1fb36e80-9156-11eb-87f1-6a660ec82a40.png)
## 차원을 늘리는 경우 문제
- 과소적합이 너무 높은 차수의 다항식은 과대적합과 모델을 느리게 하는 문제가 있다.
## 커널 트릭(Kernel trick)
- 다항식을 만들기 위한 특성을 추가하지 않으면서 수학적 기교를 적용해 다항식 특성을 추가한 것과 같은 결과를 얻을 수 있다.

## 방사기저(radial base function-**RBF**) 함수
- 커널 서포트 벡터 머신의 기본 커널 함수
- 기준점들이 되는 위치를 지정하고 각 샘플이 그 기준점들과 얼마나 떨어졌는 지를 계산 한다. => 유사도(거리)
- ![image](https://user-images.githubusercontent.com/77317312/112940575-4b3f5480-9168-11eb-95f3-020c75fac1be.png)
- `l` = 기준값
- `r`(gamma) = 하이퍼파라미터
- ![image](https://user-images.githubusercontent.com/77317312/112940692-788c0280-9168-11eb-8aad-17b2270e1b51.png)
- ![image](https://user-images.githubusercontent.com/77317312/112940713-80e43d80-9168-11eb-8f50-b31b7d780513.png)

## rbf(radial basis function) 하이퍼파라미터
- `C`
    - 오차 허용기준. 작은 값일수록 많이 허용
       - 큰값일수록 과대적하비 날 가능성이 높아진다.
    - 과대적합일 경우 값을 감소시키고, 과소적합일 경우 값을 증가 시킨다.
- `gamma`
    - 방사기저함수의 r로 규제의 역할을 한다.
        - 큰값일 수록 과대적합이 날 가능성이 높아진다.
    - 모델이 과대적합일 경우 값을 감소시키고, 과소적합일 경우 값을 증가 시킨다.
- C와 gamma는 비례관계이다.

## 정리
- 커널 기법(kernel trick)은 다항식 차수를 대신하는 효과를 가진다.
- C는 작은 값일수록 많이 허용할수 있다.
    - **과대적합**인 경우 오차허용 범위는 늘려주어야함 **C의 값을 줄인다.**
    - **과소적합**인 경우 오차허용 범위는 줄여야함 **C의 값을 늘린다.**
- gamma 방사기저함수 공식상 감마와 반환값은 **반비례관계이다.**
    - gamma가 작을수록 값들의 거리가 멀어지고(큰값이 결과로 나오므로) 클수록 거리가 가까워진다.
    - gamma가 크면 거리가 타이트해져 과적합이 일어날 수 있다.
    - svm - 비선형 분리와 rbf 테스트.ipynb 문서 확인
```python
rbf_svc = SVC(kernel='rbf', # default : rbf
              C=1, # softmargin ~ hard margin
              gamma=0.01, # rbf의 gamma
              probability=True, # True로 지정해야 predict_proba() 호출 가능
              random_state=1)

# 학습하기
rbf_svc.fit(X_train_scaled, y_train)

# 검증
pred_train = rbf_svc.predict(X_train_scaled)
pred_test = rbf_svc.predict(X_test_scaled)

accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)
# >> (0.9765258215962441, 0.965034965034965)

from sklearn.metrics import recall_score,  precision_score
recall_score(y_train, pred_train), precision_score(y_train, pred_train)
# >> (0.9962546816479401, 0.9672727272727273)

from sklearn.metrics import roc_auc_score, average_precision_score

pos_proba = rbf_svc.predict_proba(X_train_scaled)[:,1]
roc_auc_score(y_train, pos_proba), average_precision_score(y_train, pos_proba)
# >> (0.996019127034603, 0.9969714508587535)

from sklearn.model_selection import GridSearchCV
```
## GridSearch로 최적의 조합 찾기
```python
param = dict(
    kernel=['rbf', 'linear'],
    C=[0.001, 0.01, 0.1, 1, 10, 100],
    gamma=[0.001, 0.01, 0.1, 1, 10]
)

svc = SVC(random_state=1, probability=True)
gs_svc = GridSearchCV(svc,
                      param_grid=param,
                      scoring=['accuracy', 'roc_auc'],
                      refit='accuracy',
                      cv=3,
                      n_jobs=-1)

gs_svc.fit(X_train_scaled, y_train)

print(gs_svc.best_params_)
# >> {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}
```
