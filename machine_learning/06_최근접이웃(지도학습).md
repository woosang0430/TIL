# K-Nearest Neighbors,  K-NN (K-최근접 이웃)
- 분류(Classification)와 회귀(Regression)를 모두 지원
- 학습시 단순히 input 데이터들을 저장만 하며 예측 시점에 거리를 계산
  - 학습은 빠르지만 예측은 오래 걸림
- 서비스를 직접 만들지 않고 데모? 구현할때 빠르게 만들어본다.
- ![image](https://user-images.githubusercontent.com/77317312/112784337-5fae1f00-908c-11eb-91a1-f21a6b2f077e.png)
- k를 1로 하면 **파란색**, k를 3으로 하면 **주황색**으로 분류
- k가 너무 작으면 **overfitting**, 너무 크면 **underfitting**

## 1. 주요 하이퍼 파라미터
- 이웃 수
  - `n_neighbors` = k
  - **k가 너무 작으면 **overfitting**, 너무 크면 **underfitting****
- 거리 재는 방법
  - `p=2` : 유클리디안 거리(Euclidean distance - default)
  - `p=1` : 맨하탄 거리(manhattan distance)
> - 유클리디안 거리(Euclidean_distance, L2 norm)
> - ![image](https://user-images.githubusercontent.com/77317312/112809317-db23c680-90b4-11eb-8be4-17f6979c86b7.png)

> - 맨하탄 거리 (Manhattan distance, L1 norm)
> - ![image](https://user-images.githubusercontent.com/77317312/112809421-f55da480-90b4-11eb-8eae-5753515af5f5.png)

## 요약
- K-NN은 이해하기 쉬운 모델이며 튜닝할 하이퍼파라미터의 수가 적어 빠르게 만들 수 있다.
- 서비스 모델 구현보다는 **복잡한 알고리즘을 적용해 보기 전에 확인용 또는 base line을 잡기 위한 모델로 사용**
- Feature간의 값의 단위가 다르면 작은 단위의 Feature에 영향을 많이 받게 되므로 **전처리로 Scaling작업**이 필요
- Feature가 너무 많은 경우와 대부분의 값이 0으로 구성되(희소-sparse)  데이터셋에서 성능이 아주 나쁘다.

## 위스콘신 유방암 데이터를 이용한 암환자 분류
- k 값 변환에 따른 성능 평가
- `malignant` : 악성, `benign` : 양성
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler


pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())

param_grid = dict(
  kneighborsclassifier__n_neighbors=range(1, 21)
)

gs = GridSearchCV(pipeline,
                  param_grid=param_grid,
                  scoring='accuracy',
                  cv=3,
                  n_jobs=-1)
# 학습
gs.fit(X_train, y_train)

# 검증
pred_train = gs.predict(X_train)
pred_test = gs.predict(X_test)

accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)
```
















