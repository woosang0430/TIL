# 앙상블의 종류
1. 투표방식
- 여러개의 추정기(estimator)가 낸 결과들을 투표를 통해 최종 결과를 내는 방식
  - `Bagging` - 같은 모델을 조합하되 각각 학습하는 데이터를 다르게 한다.
  - `Voting` - 서로 다른 종류의 알고리즘을 결합
2. 부스팅(Boosting)
  - 약한 학습기(week Learner)들을 결합해서 보다 정확하고 강한 학습기(strong learner)를 만든다.

# Voting
## voting의 유형
1. hard voting
- 다수의 추정기가 결정한 예측값들 중 많은 것을 선택하는 방식
- ![image](https://user-images.githubusercontent.com/77317312/113086929-97000580-921d-11eb-9373-721b7482b8b8.png)
2. soft voting
- 다수의 추정기에서 각 레이블별 예측한 확률들의 평균을 내서 높은 레이블값을 결과값으로 선택하는 방식
- ![image](https://user-images.githubusercontent.com/77317312/113087006-b72fc480-921d-11eb-80c5-a92629965379.png)
> - 일반적으로 soft voting이 성능이 더 좋다.
> - Voting은 성향이 다르면서 비슷한 성능을 가진 모델들을 묶었을 때 가장 좋은 성능을 낸다.

## VotingClassifier 클래스 이용
- parameter
  - `estimators` : 앙상블한 모델들 설정. `('추정기 이름', 추정기)`의 튜플을 리스트로 묶어 전달
  - `voting` : hard(default), soft -> voting 방식
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# wine data
wine = pd.read_csv('data/wine.csv')

y = wine['color']
X = wine.drop('color', axis=1)

# OneHotEncoding을 위해 전처리 ==> 범주형 컬럼의 값을 숫자로 
# 컬럼의 값의 크기에 영향을 받는 모델들이 있으므로 OneHotEncoding 사용

ohe = OneHotEncoder()
quality = ohe.fit_transform(np.array(X['quality']).reshape(-1, 1)).toarray()

# 데이터 프레임으로 변환 후 붙이기
X = pd.concat([X, pd.DataFrame(quality, columns=ohe.get_feature_names())], axis=1)

# 원래있던 quality컬럼 제외
X.drop('quality', axis=1, inplace=True)

# train, test set 나누기
X_train, X_test, y_train, t_test = train_test_split(X, y, stratift=y, random_state=1)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

knn = KNeighborsClassifier(n_neighbors=5)
rf = RandomForestClassifier(n_estimators=200)
svc = SVC(C=1.0, gamma=0.1, probability=True) 
# soft voing일때 probability를 True로 주지않으면 검증하는 곳에서 에러가 난다

# estimators = [('knn', knn), 
#               ('random forest', rf), 
#               ('svm', svc)]

# knn, svc는 scaling을 처리. rf는 처리안함
from sklearn.pipeline import make_pipeline

knn_pipe = make_pipeline(StandardScaler(), knn)
svc_pipe = make_pipeline(StandardScaler(), svc)
estimators = [('knn', knn_pipe), 
              ('random forest', rf), 
              ('svm', svc_pipe)]

voting = VotingClassifier(estimators)

# 검증 함수 정의
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

def print_metrics(y, y_pred, title=None):
    if title: # title != None
        print(title)
    metrics = f'정확도 : {accuracy_score(y, y_pred)}, 재현율:{recall_score(y,  y_pred)}, f1점수:{f1_score(y, y_pred)}'
    print(metrics)
```
```python
# hard voting
voting.fit(X_train, y_train) # estimators에 등록할 모든 모델을 학습시킨다.

pred_train = voting.predict(X_train)
pred_test = voting.predict(X_test)

print_metrics(y_train, pred_train, 'train 데이터셋-Hard Voting')
# >> train 데이터셋-Hard Voting
# >> 정확도 : 0.9967159277504105, 재현율:0.9883236030025021, f1점수:0.9932942162615256

print_metrics(y_test, pred_test, 'test 데이터셋-Hard Voting')
# >> test 데이터셋-Hard Voting
# >> 정확도 : 0.9969230769230769, 재현율:0.9925, f1점수:0.9937421777221527
```
```python
# soft voting : class별 확률의 평균으로 결정.
voting_soft = VotingClassifier(estimators, voting='soft') # default: 'hard'

# 학습
voting_soft.fit(X_train, y_train)

# 검증
pred_train_soft = voting_soft.predict(X_train)
pred_test_soft = voting_soft.predict(X_test)

print_metrics(y_train, pred_train_soft, 'trainset-soft Voting')
# >> trainset-soft Voting
# >> 정확도 : 0.9967159277504105, 재현율:0.9883236030025021, f1점수:0.9932942162615256

print_metrics(y_test, pred_test_soft, 'testset-soft Voting')
# >> testset-soft Voting
# >> 정확도 : 0.9969230769230769, 재현율:0.9925, f1점수:0.9937421777221527
```
