# Boosting
- 단순하고 약한 학습기(weak  learner)들을 결합해서 보다 정확하고 강력한 학습기(strong learner)를 만드는 방식
- 약한 학습기들은 앞 학습기가 만든 오류를 줄이는 방향으로 학습한다.
- gradient boosting
  - 앞 학습기가 만든 오류를 앞 모델에 update 하면서 오류를 줄이는 과정
  - 오류를 update할 때 뺄까 더할까를 gradient descent 방법을 쓴다.
  - 미분해서 나오는 값의 음수를 취해서 적용
  
## Gradient Boosting
- 개별 모델로 **Decision Tree** 를 사용한다.
- depth가 깊지 않은 트리를 많이 연결해서 이전 트리의 오차를 보정해 나가는 방식
- 오차를 보정할 때 **경사하강법** 사용
- 분류와 회귀 둘다 지원하는 모델
- 훈련시간이 많이 걸리고, 트리기반 모델의 특성상 **희소한 고차원 데이터**에서는 성능이 안좋은 단점이 있다.
- **희소한 고차원 데이터** : 차원수도 많고 0이 많은 것

## 주요 파라미터
- Decision Tree의 가지치기 관련 매개변수
  - 각각의 tree가 복잡한 모델이 되지 않도록한다. ==> depth의 값은 작게 한다.
- `learning rate` default : 0.1
  - 이전 tree의 오차를 얼마나 강하게 보정할 것인지 제어하는 값
  - 값이 크면 보정을 강하게 하여 복잡한 모델을 만든다.
      - 과대적합이 날 수 있다.
  - 값을 작게 잡으면 보정을 약하게 하여 모델의 복잡도를 줄인다.
      - 과대적합을 줄일 수 있지만 성능 자체가 낮아질 수 있다.
- `n_estimators`
  - tree의 개수 지정. 많을 수록 모델이 복잡해진다.
- `n_iter_no_change`, `validation_fraction`
  - `validation_fraction`에 지정한 비율만큼 `n_iter_no_change`에 지정한 반복 횟수동안 검증점수가 좋아 지지않으면 훈련 조기종료
- 보통 max_depth를 낮춰 개별 트리의 복잡도를 낮춘다.(5가 넘지 않게) 그리고 n_estimators를 가용시간, 메모리 한도에 맞춘뒤 적절한 learning_rate을 찾는다.
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# 데이터 불러오기
cancer = load_breast_cancer()
X, y = cancer['data'], cancer['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)
y_train.shape, y_test.shape

# gradientboosting 호출 및 학습
gb = GradientBoostingClassifier(random_state=1)

gb.fit(X_train, y_train)

# 검증
pred_train = gb.predict(X_train)
pred_test = gb.predict(X_test)

accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)
# >> (1.0, 0.958041958041958)

# feature 중요도
import pandas as pd
fi = gb.feature_importances_
fi_s = pd.Series(fi, cancer['feature_names'])

fi_s.sort_values(ascending=False)
# >> worst radius               0.383871
# >> worst concave points       0.285990
# >> worst perimeter            0.130654
# >> mean concave points        0.046544
# >> worst area                 0.042472
# >> ...
```
## GridSearchCV 이용해 최적의 하이퍼파라미터 찾기
```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
paran = dict(
  n_estimators=[100, 200, 300, 400, 500], # tree개수 (default:100)
  learning_rate=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5], # 학습률
  max_depth=range(1, 5),
  subsample=[0.5, 0.7, 1]  # 학습시킬 sample의 비율
)
gb = GradientBoostingClassifier(random_state=1)

gs = GridSearchCV(gb,
                  param_grid=param,
                  cv=3,
                  scoring='accuracy',
                  n_jobs=-1)
# 학습
gs.fit(X_train, y_train)

# 베스트 파라미터
gs.best_params_
# >> {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}

# 검증
pred_train = gs.predict(X_train)
pred_test = gs.predict(X_test)

accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)
# >> (1.0, 0.958041958041958)

model = gs.best_estimator_
model.feature_importances_ # GridSearch에는 feature_importance가 없기 때문에 뽑아와서 확인
# >> array([0.00306088, 0.01792034, 0.00119596, ...])
```
## XGBoost(Extra Gradient Boost)
- https://xgboost.readthedocs.io/
- Gradient Boost 알고리즘을 기반으로 개선해서 나온 모델.
- Gradient Boost의 단점인 느린수행시간을 해결하고 과적합을 제어할 수 있는 규제를 제공하여 성능을 높임.
- 두가지 개발 방법
    - [Scikit-learn 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)
    - [파이썬 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)
- 설치   
``
pip install xgboost
conda install -y -c anaconda py-xgboost
``
## Scikit-learn 래퍼 XGBoost
- 주요 매개변수
  - `learning_rate` : 학습률, 보통 0.01 ~ 0.2 사이의 값 사용
  - `n_estimators` : week tree 개수
  - `max_depth` : tree의 depth 지정
```python
from xgboost import XGBClassifier

xgb = XGBClassifier(n_estimators=200, 
                    learning_rate=0.5,
                    max_depth=2, 
                    random_state=1)
                    
xgb.fit(X_train, y_train)                   

pred_train = xgb.predict(X_train)
pred_test = xgb.predict(X_test)

accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)
# >> (1.0, 0.965034965034965)
```
