## 분류와 회귀의 평가방법
- `sklearn.metrics` 모듈을 통해 제공

#### 분류 평가지표 => 분균형 data_set일 때 본다(이진분류)
> 1. 정확도(accuracy)
> 2. 정밀도(Precision)
> 3. 재현률 / 민감도(Recall)
> 4. F1점수 (F1 Score)
> 5. PR Curve, AP
> 6. ROC, AUC

#### 회귀 평가방법
> 1. MSE (Mean Squard Error)
> 2. RMSE (Root Mean Squard Error)
> 3. R**2 (결정계수)

## 분류(Classification) 평가 기준
#### **용어**
- 이진 분류에서 양성(Positive)과 음성(Negative)
  - 양성 : 예측하려는 대상
  - 음성 : 예측하려는 대상이 아닌 것
  - Ex)
    - 암환자 분류 : 양성 - 암 환자, 음성 - 정상인
    - 스팸메일 분류 : 양성 - 스팸메일, 음성 - 정상메일
    - 금융사기 모델 : 양성 - 사기거래, 음성 - 정상거래
#### 정확도(accuracy)
- ![image](https://user-images.githubusercontent.com/77317312/112122957-24cb6780-8c04-11eb-825a-22f32564e7e4.png)
- 전체 예측 한 것중 맞게 예측한 비율 평가
- `accuracy_score(정답, 모델 예측값)`
#### Accuracy 평가지표의 문제
- 불균형 데이터의 경우 정확한 지표가 될 수 없음
- ex) 양성과 음성의 비율이 1:9의 경우 모두 음성이라고 하면 정확도는 90%

### 실습(MNIST Data Set)
- 손글씨 데이터 셋
- 사이킬런 제공 image size : 8 x 8
- https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

digits = load_digits()
X, y = digits['data'], digits['target']

plt.figure(figsize=(5, 5))
for i in range(9):
  plt.subplot(3, 3, i+1)
  plt.imshow(X[i].reshape(8, 8), cmap='Greys')
  plt.xticks([])
  plt.yticks([])
  plt.title(y[i], fontdict={'fontsize':25})
  
plt.tight_layout()
plt.show()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112124098-54c73a80-8c05-11eb-8c17-663c71caf311.png)

### 불균형 데이터셋으로 만들기
- y를 9와 나머지를 변경한다.
- Positive(양성 -> 1) : 9
- Negative(음성 -> 0) : 0 ~ 8
```python
y = (y == 9) # 9 : True, 나머지 : False -> boolean은 내부적으로 0, 1로 처리된다.

cnt2 = np.unique(y, return_counts=True)
cnt2
# >> (array([False,  True]), array([1617,  180], dtype=int64))

# 훈련, 테스트 데이터셋 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
print(np.unique(y_train, return_counts=True)[1]/y_train.size) # False, True 비율
print(np.unique(y_test, return_counts=True)[1]/y_test.size) # False, True 비율
# >> [0.89979123 0.10020877]
# >> [0.9 0.1]
```
#### 모델 생성 및 학습
- **Dummy Model** 정의
- Target label중 무조건 최빈값으로 예측하는 모델 정의
- 그냥 모델 생성되는 Flow만 보자
```python
from sklearn.base import BaseEstimator

class MyModel(BaseEstimator):

  def fit(self, x, y):
    cnt = np.unique(y, return_counts=True)
    max_idx = cnt[1].argmax() # max값의 index 반환
    self.pred  = cnt[0][max_idx] # 최빈값을 instance 변수로 저장
  
  def predict(self, X):
    return np.full(shape=(X.shape[0], 1), fill_value=self.pred)
    
model = MyModel()
model.fit(X_train, y_train)
pred_train = model.predict(X_train)
pred_test = model.predict(X_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_train, pred_train))
print(accuracy_score(y_test, pred_test))
# >> 0.8997912317327766
# >> 0.9
```
## => 위의 정의된 True, False의 비율과 같은 값이 나온다.
- 상황에 따라 accuracy_score만으로는 정확한 검증이 어렵다.

## 1. 혼동 행렬(Confusion Marix)
- 분류의 평가지표의 기준으로 사용
- 혼동행렬을 이용해 다양한 평가지표(정확도, 재현률, 정밀도, F1 점수, AUC 점수)를 계산할 수 있다.
- 함수 `confusion_matrix(정답, 모델예측값)`
- 결과의 0번 축 : 실제(Ground Truth), 1번 축 : 예측 class
- ![image](https://user-images.githubusercontent.com/77317312/112126454-d15b1880-8c07-11eb-9718-09a09ce4367a.png)
- ![image](https://user-images.githubusercontent.com/77317312/112126512-dddf7100-8c07-11eb-96d0-199df7e4bb00.png)
- TP(True Positive) : 양성으로 예측했는데 맞은 개수
- TN(True Negative) : 음성으로 예측했는데 맞은 개수
- FP(False Positive) : 양성으로 예측했는데 틀린 개수 (음성을 양성으로 예측)
- FN(False Negative) : 음성으로 예측했는데 틀린 개수 (양성을 음성으로 예측)
## 2. 불균형에서 recall, precision 자주 사용
#### 이진 분류 평가 점수
1. Accuracy(정확도)
>  - 전체 데이터 중에 맞게 예측한 것의 비율

2. Recall / Sensitivity(재현율 / 민감도)
>  - 실제 Positive인 것 중에 Positive로 예측 한 것의 비율
>  - **TPR**(True Positive Rate) => 기억해두기
>  - ex) 스팸 메일 중 스팸메일로 예측한 비율. 금융사기 데이터 중 사기로 예측한 비율

3. precision(정밀도)
>  - Positive로 예측 한 것 중 실제 Positive인 비율
>  - **PPV**(Positive Predictive Value)라고도 함
>  - ex) 스팸메일로 예측한 것 중 스팸메일의 비율. 금융 사기로 예측한 것 중 금융사기인 것의 비율

4. F1 점수
>  - 정밀도와 재현율의 조화평균 점수
>  - recall과 precision이 비슷할 수록 높은 값을 가진다.

5. 기타.
> - Specificity(특이도)
>   - 실제 Negative인 것들 중 Negative으로 맞게 예측한 것의 비율
>   - TNR(True Negative Rate)
> - Fall out(위양성률)
>   - 실제 Negative인 것들 중 Positive으로 잘못 예측한 것의 비율
>   - **FPR**(False Positive Rate) => 기억해두기
>   - ![image](https://user-images.githubusercontent.com/77317312/112128234-8f32d680-8c09-11eb-971e-030b0995dbbc.png)
- ![image](https://user-images.githubusercontent.com/77317312/112128290-a07be300-8c09-11eb-9b1d-eb6296e16b12.png)
### 각 평가 지표 계산 함수
- `sklearn.metrics` 모듈
- **confusion_matrix(y 실제값, y 예측값)**
  - 혼돈 행렬 반환
- **recall_score(y 실제값, y 예측값)**
  - recall(재현율) 점수 반환 => TPR
- **precision_score(y 실제값, y 예측값)**
  - precision(정밀도) 점수 반환 => PPV
- **f1_score(y 실제값, y 예측값)**
  - F1 점수 반환 (recall과 precision의 조화 평균값)
- **classification_report(y 실제값, y 예측값)**
  - 클래스 별로 recall, precision, f1 점수와 accuracy를 종합해서 보여준다.
> - Dummy 모델 혼동행렬
>   - plot_confusion_matrix함수 : 버전 2.1.3에서  추가됨, 없다고 에러나는 경우 업데이트 ㄱㄱ
>   - `pip install scikit-learn --upgrade`
```python
from sklearn.metrics import confusion_matrix, plot_confusion_matrix
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score

print('train confusion matrix')
print(confusion_matrix(y_train, pred_train))
print('-' * 50)
print('test confusion matrix')
print(confusion_matrix(y_test, pred_test))
# >> Train confusion matrix
# >> [[1293    0]
# >>  [ 144    0]]
# >> ----------------------
# >> Test confusion matrix
# >> [[324   0]
# >>  [ 36   0]]

# 더미 모델 생성
from sklearn.dummy import DummyClassifier

dummy_model = DummyClassifier(stratigy='most_frequent')
dummy_model.fit(X_train, y_train)

# hitmap 그리기
fig, ax = plt.subplot(1, 1, figsize=(5,5))
plot_confusion_matrix(dummy_model, # 학습 모델
                      X_train,
                      y_train,
                      display_labels=['Neg', 'Pos'],
                      cmap='Blues',
                      ax=ax)
plt.show()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112239134-40c51c80-8c89-11eb-858c-19e1e9724908.png)
#### classification_report()
```python
from sklean.metrics import classification_report
result = classification_report(y_test, pred_test_tree)
print(result) # print하면 이쁘게 나옴
# >>               precision    recall  f1-score   support
# >> 
# >>        False       0.97      0.96      0.96       324
# >>         True       0.66      0.69      0.68        36
# >> 
# >>     accuracy                           0.93       360
# >>    macro avg       0.81      0.83      0.82       360
# >> weighted avg       0.94      0.93      0.93       360
```
## 3. 재현율과 정밀도의 관계
- 이진 분류의 경우 Precision(정밀도)가 중요한 경우와 recall(재현율)이 중요한 업무가 각각 있다.

#### 재현율이 더 중요한 경우(recall)
- 실제 Positive 데이터를 Negative로 잘못 판단하면 업무상 큰 영향이 있는 경우
- **FN(false negative)** 를 낮추는데 초점을 맞춘다.
- **암환자 판정 모델**, **보험사기적발** 모델
- `애매할 때는 **positive**로 예측해` 라고 생각하자

#### 정밀도가 더 중요한 경우(precision)
- 실제 Negative 데이터를 Positive로 잘못 판단하면 업무상 큰 영향이 있는 경우
- **FP(false positive)** 를 낮추는데 초점을 맞춘다.
- **스팸메일** 판정
- `애매할 때는 **Negative**로 예측해!` 라고 생가가하자

##### ===> 정밀도와 재현율 중 한가지를 높이면 다른 하나는 낮아지는 상충관계이다.

#### 임계값(Threshold) 변경을 통한 재현율, 정밀도 변환
- 임계값 : 모델이 분류의 답을 결정할 때 기준값
- 정밀도나 재현율을 특히 강조해야 하는 상황일 경우 임계값 변경을 통해 평가 수치를 올릴 수 있다.
- 단 극단적으로 임계점을 올리거나 낮춰서 한쪽의 점수를 높이면 안된다. (ex. 암환자 예측시 재현율을 너무 높이면 정밀도가 낮아져 걸핏하면 정상인을 암환자로 예측하게 된다.)

#### 임계값 변경에 따른 정밀도와 재현율 변화관계
- **임계값을 낮추면 재현율은 올라가고 정밀도는 낮아진다.**
- **임계값을 높이면 재현율은 낮아지고 정밀도는 올라간다.**
  - 임계값을 변화시켰을 때 **재현율과 정밀도는 음의 상관관계**를 가진다.
  - 임계값을 변화시켰을 때 **재현율과 위양성율(fall-out/fpr)은 양의 상관관계**를 가진다.
- ![image](https://user-images.githubusercontent.com/77317312/112268766-d4631100-8cba-11eb-96bf-721734e7e3e7.png)
#### 임계값 변화에 따른 recall, precision 변화
```python
from sklearn.metrics import precision_recall_curve

pos_proba = tree.predict_probar(X_test)[:, 1] # 모든 행의 1번 컬럼만
precisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba)
precisions.shape, recalls.shape, thresholds.shape
# >> ((8,), (8,), (7,))

# 시각화
from matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
plt.plot(thresholds, precisions[:-1], label='precision')
plt.plot(thresholds, recalls[:-1], label='recall')

plt.grid(True)
plt.legend()
plt.show()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112269583-e6917f00-8cbb-11eb-85dd-97b1a1324cd0.png)

### Binarizer - 임계값 변경
- transformer로 양성 여부를 선택하는 임계값을 변경할 수 있다.
```python
from sklearn.preprocessing import Binarizer

example = [[0.3, 0.5, 0.7, 0.4, 0.6]]
bi = Binarizer(threshold=0.5) # 임계값 지정
bi.fit(example)
bi.transform(example)
# >> array([[0., 0., 1., 0., 1.]])

# 머신러닝 모델에 적용
pos_proba = tree.predict_proba(X_test) # posivite들의 확률
binarizer = Binarizer(threshold=0.5)

binarizer.fit(pos_proba)
predict = binarizer.transform(pos_proba)[:, 1]
recall_score(y_test, predcit), precision_score(y_test, predict)
# >> (0.8055555555555556, 0.7837837837837838)
```

## 4. PR Curve(precision Recall Curve - 정밀도 재현율 곡선), AP Score(Average Precision Score)
- 0 ~ 1사이의 모든 임계값에 대하여 재현율(recall)과 정밀도(rpecision)의 변화를 이용한 평가 지표
- X축에 재현율, Y축에 정밀도를 놓고 임계값이 0 -> 1 변화할때 두 값의 변화를 선 그래프로 그린다.
- AP Score
  - PR Curve의 성능평가 지표를 하나의 점수로 평가한 것
  - PR Curve의 선아래 면적을 계산한 값으로 높을 수록 성능이 good
- ![image](https://user-images.githubusercontent.com/77317312/112277184-d500a500-8cc4-11eb-91a1-75161daff160.png)
```python
from sklearn.metrics import precsion_recall_curve, plot_precision_recall_curve
from sklearn.metrics import average_precision_score

pos_proba = tree.predict_proba(X_test)[:, 1]

precisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba)
precisions.shape, recalls.shape, thresholds.shape
# >> ((9,), (9,), (8,))

# 값 맞추기
thresholds = np.append(thresholds, 1)

# AP score
average_precision_score(y_test, pos_proba)
# >> 0.7223042051700589

# 그래프 그리기
fig, ax = plt.subplots(1, 1, figsize=(5, 5))
# plt.gca() # 현재 그리는 axes를 뽑을 수 있다.

plot_precision_recall_curve(tree,
                            X_test,
                            y_test,
                            ax=ax)
plt.grid(True)
plt.show()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112278139-e0080500-8cc5-11eb-926b-4d88e2d0bc75.png)

## 5. ROC Curve(Receiver Operating Characteristic Curve), AUC(Area Under the curve) score
- **FPR(False Positive Rate)** -> 위양성율
  - 위양성율 (fall-out)
  - 1-특이도(TNR)
  - 실제 음성중 양성으로 잘못 예측 한 비율
  - 증가하면 실제 negative에서 틀린 것이 많다.
  - ![image](https://user-images.githubusercontent.com/77317312/112279140-e9de3800-8cc6-11eb-91ab-e9b7208b7d0e.png)
- **TPR(True Positive Rate)** -> 재현율/민감도
  - 재현율(recall)
  - 실제 양성중 양성으로 맞게 예측한 비율
  - 증가하면 실제 Positive에 맞은게 많다.
  - ![image](https://user-images.githubusercontent.com/77317312/112279274-198d4000-8cc7-11eb-86d2-8938724c9fa8.png)
- **Threshold의 변화에 따른 영향을 받는 친구들**
  - TPR과 FPR -> 양의 상관관계(비례관계)
  - FPR이 낮고 TPR이 높은 모델이 좋은 것
- **ROC 곡선**
  - 2진 분류의 모델 성능 평가 지표 중 하나
  - 불균형 데이터셋을 평가할 때 사용
  - [FPR: X축, TPR: Y축]임계값을 변경해서 FPR이 변할 때 TPR이 어떻게 변하는지 나타내는 곡선
- **AUC**
  - ROC 곡선 아래쪽 면적
  - 0 ~ 1 사이 실수로 나옴
  - 점수 기준
  
  | 점수 | 1.0 ~ 0.9 | 0.9 ~ 0.8| 0.8 ~ 0.7 | 0.7 ~ 0.6 | 0.6 ~ 0.5 |
  | -- | -- | -- | -- | -- | -- |
  | 정도 | 아주 좋음 | 좋음 | 괜찮은 모델 | 좀 그럼.. | 좋지 않은 모델 |
- ![image](https://user-images.githubusercontent.com/77317312/112281048-fa8fad80-8cc8-11eb-8f8e-9691af47608b.png)
- 가장 완벽한 것은 FPR이 0이고 TRP이 1인 것
- 일반적으로 FPR이 작을 때(0에 가까울 때) TPR이 높은 경우가 좋은 상황
#### ROC, AUC 점수 확인
- `roc_curve(y값, 예측확률)` : FPR, TPR, Thresholds(임계치) 반환
- `roc_auc_score(y값, 예측확률)` : AUC 점수 반환

## 5-1. ROC Curve - PR Curve
- `ROC` : 이진분류에서 양성클래스 탐지와 음성클래스 탐지의 중요도가 비슷할 때 사용(개, 고양이 분류)
- `PR curve(precision Recall)` : 양성 클래스 탐지가 음성클래스 탐지의 중요도보다 높을 경우 사용(암환자 진단)
```python
from sklearn.metrics import roc_curve, plot_roc_curve, roc_auc_score

pos_proba_tree = tree.predict_proba(X_test)[;, 1]
pos_proba_rf = rf.predict_proba(X_test)[:, 1]

fpr_tree, tpr_tree, threshold_tree = roc_curve(y_test, pos_proba_tree)
fpr_rf, tpr_rf, threshold_rf = roc_curve(y_test, pos_proba_rf)

pd.DataFrame(dict(th:threshold_tree, fpr=fpr_tree, tpr=tpr_tree))
```
- output

|th	| fpr	| tpr|
| -- | -- | -- |
|0	| 1.761538	| 0.000000	| 0.000000 |
|1	| 0.761538	| 0.024691	| 0.805556 |
|2	| 0.500000	| 0.027778	| 0.888889 |
|3	| 0.428571	| 0.030864	| 0.888889 |
|4	| 0.140000	| 0.058642	| 0.916667 |
|5	| 0.037037	| 0.083333	| 0.916667 |
|6	| 0.024096	| 0.225309	| 0.916667 |
|7	| 0.015432	| 0.975309	| 0.944444 |
|8	| 0.000000	| 1.000000	| 1.000000 |

```python
# roc_auc_score
print('tree :',roc_auc_score(y_test, pos_proba_tree))
print('rf :',roc_auc_score(y_test, pos_proba_rf))
# >> tree : 0.9150805898491085
# >> rf : 0.9993569958847736

# 그래프 그리기
plt.figure(figsize=(5, 5))
ax = plt.gca()
plot_roc_curve(tree,
               X_test,
               y_test,
               ax=ax)

plot_roc_curve(rf,
               X_test,
               y_test,
               ax=ax)
plt.show()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112283172-33308680-8ccb-11eb-81b3-a749b3c7ad94.png)
