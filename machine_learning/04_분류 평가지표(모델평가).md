## 분류와 회귀의 평가방법
- `sklearn.metrics` 모듈을 통해 제공

#### 분류 평가지표 => 분균형 data_set일 때 본다(이진분류)
> 1. 정확도(accuracy)
> 2. 정밀도(Precision)
> 3. 재현률 / 민감도(Recall)
> 4. F1점수 (F1 Score)
> 5. PR Curve, AP
> 6. ROC, AUC

#### 회귀 평가방법
> 1. MSE (Mean Squard Error)
> 2. RMSE (Root Mean Squard Error)
> 3. R**2 (결정계수)

## 분류(Classification) 평가 기준
#### **용어**
- 이진 분류에서 양성(Positive)과 음성(Negative)
  - 양성 : 예측하려는 대상
  - 음성 : 예측하려는 대상이 아닌 것
  - Ex)
    - 암환자 분류 : 양성 - 암 환자, 음성 - 정상인
    - 스팸메일 분류 : 양성 - 스팸메일, 음성 - 정상메일
    - 금융사기 모델 : 양성 - 사기거래, 음성 - 정상거래
#### 정확도(accuracy)
- ![image](https://user-images.githubusercontent.com/77317312/112122957-24cb6780-8c04-11eb-825a-22f32564e7e4.png)
- 전체 예측 한 것중 맞게 예측한 비율 평가
- `accuracy_score(정답, 모델 예측값)`
#### Accuracy 평가지표의 문제
- 불균형 데이터의 경우 정확한 지표가 될 수 없음
- ex) 양성과 음성의 비율이 1:9의 경우 모두 음성이라고 하면 정확도는 90%

### 실습(MNIST Data Set)
- 손글씨 데이터 셋
- 사이킬런 제공 image size : 8 x 8
- https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

digits = load_digits()
X, y = digits['data'], digits['target']

plt.figure(figsize=(5, 5))
for i in range(9):
  plt.subplot(3, 3, i+1)
  plt.imshow(X[i].reshape(8, 8), cmap='Greys')
  plt.xticks([])
  plt.yticks([])
  plt.title(y[i], fontdict={'fontsize':25})
  
plt.tight_layout()
plt.show()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112124098-54c73a80-8c05-11eb-8c17-663c71caf311.png)

### 불균형 데이터셋으로 만들기
- y를 9와 나머지를 변경한다.
- Positive(양성 -> 1) : 9
- Negative(음성 -> 0) : 0 ~ 8
```python
y = (y == 9) # 9 : True, 나머지 : False -> boolean은 내부적으로 0, 1로 처리된다.

cnt2 = np.unique(y, return_counts=True)
cnt2
# >> (array([False,  True]), array([1617,  180], dtype=int64))

# 훈련, 테스트 데이터셋 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
print(np.unique(y_train, return_counts=True)[1]/y_train.size) # False, True 비율
print(np.unique(y_test, return_counts=True)[1]/y_test.size) # False, True 비율
# >> [0.89979123 0.10020877]
# >> [0.9 0.1]
```
#### 모델 생성 및 학습
- **Dummy Model** 정의
- Target label중 무조건 최빈값으로 예측하는 모델 정의
- 그냥 모델 생성되는 Flow만 보자
```python
from sklearn.base import BaseEstimator

class MyModel(BaseEstimator):

  def fit(self, x, y):
    cnt = np.unique(y, return_counts=True)
    max_idx = cnt[1].argmax() # max값의 index 반환
    self.pred  = cnt[0][max_idx] # 최빈값을 instance 변수로 저장
  
  def predict(self, X):
    return np.full(shape=(X.shape[0], 1), fill_value=self.pred)
    
model = MyModel()
model.fit(X_train, y_train)
pred_train = model.predict(X_train)
pred_test = model.predict(X_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_train, pred_train))
print(accuracy_score(y_test, pred_test))
# >> 0.8997912317327766
# >> 0.9
```
## => 위의 정의된 True, False의 비율과 같은 값이 나온다.
- 상황에 따라 accuracy_score만으로는 정확한 검증이 어렵다.

## 1. 혼동 행렬(Confusion Marix)
- 분류의 평가지표의 기준으로 사용
- 혼동행렬을 이용해 다양한 평가지표(정확도, 재현률, 정밀도, F1 점수, AUC 점수)를 계산할 수 있다.
- 함수 `confusion_matrix(정답, 모델예측값)`
- 결과의 0번 축 : 실제(Ground Truth), 1번 축 : 예측 class
- ![image](https://user-images.githubusercontent.com/77317312/112126454-d15b1880-8c07-11eb-9718-09a09ce4367a.png)
- ![image](https://user-images.githubusercontent.com/77317312/112126512-dddf7100-8c07-11eb-96d0-199df7e4bb00.png)
- TP(True Positive) : 양성으로 예측했는데 맞은 개수
- TN(True Negative) : 음성으로 예측했는데 맞은 개수
- FP(False Positive) : 양성으로 예측했는데 틀린 개수 (음성을 양성으로 예측)
- FN(False Negative) : 음성으로 예측했는데 틀린 개수 (양성을 음성으로 예측)
## 2. 불균형에서 recall, precision 자주 사용
#### 이진 분류 평가 점수
1. Accuracy(정확도)
>  - 전체 데이터 중에 맞게 예측한 것의 비율

2. Recall / Sensitivity(재현율 / 민감도)
>  - 실제 Positive인 것 중에 Positive로 예측 한 것의 비율
>  - **TPR**(True Positive Rate) => 기억해두기
>  - ex) 스팸 메일 중 스팸메일로 예측한 비율. 금융사기 데이터 중 사기로 예측한 비율

3. precision(정밀도)
>  - Positive로 예측 한 것 중 실제 Positive인 비율
>  - **PPV**(Positive Predictive Value)라고도 함
>  - ex) 스팸메일로 예측한 것 중 스팸메일의 비율. 금융 사기로 예측한 것 중 금융사기인 것의 비율

4. F1 점수
>  - 정밀도와 재현율의 조화평균 점수
>  - recall과 precision이 비슷할 수록 높은 값을 가진다.

5. 기타.
> - Specificity(특이도)
>   - 실제 Negative인 것들 중 Negative으로 맞게 예측한 것의 비율
>   - TNR(True Negative Rate)
> - Fall out(위양성률)
>   - 실제 Negative인 것들 중 Positive으로 잘못 예측한 것의 비율
>   - **FPR**(False Positive Rate) => 기억해두기
>   - ![image](https://user-images.githubusercontent.com/77317312/112128234-8f32d680-8c09-11eb-971e-030b0995dbbc.png)
- ![image](https://user-images.githubusercontent.com/77317312/112128290-a07be300-8c09-11eb-9b1d-eb6296e16b12.png)
### 각 평가 지표 계산 함수
- `sklearn.metrics` 모듈
- **confusion_matrix(y 실제값, y 예측값)**
  - 혼돈 행렬 반환
- **recall_score(y 실제값, y 예측값)**
  - recall(재현율) 점수 반환 => TPR
- **precision_score(y 실제값, y 예측값)**
  - precision(정밀도) 점수 반환 => PPV
- **f1_score(y 실제값, y 예측값)**
  - F1 점수 반환 (recall과 precision의 조화 평균값)
- **classification_report(y 실제값, y 예측값)**
  - 클래스 별로 recall, precision, f1 점수와 accuracy를 종합해서 보여준다.
> - Dummy 모델 혼동행렬
>   - plot_confusion_matrix함수 : 버전 2.1.3에서  추가됨, 없다고 에러나는 경우 업데이트 ㄱㄱ
>   - `pip install scikit-learn --upgrade`
```python
from sklearn.metrics import confusion_matrix, plot_confusion_matrix
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score

print('train confusion matrix')
print(confusion_matrix(y_train, pred_train))
print('-' * 50)
print('test confusion matrix')
print(confusion_matrix(y_test, pred_test))
# >> Train confusion matrix
# >> [[1293    0]
# >>  [ 144    0]]
# >> ----------------------
# >> Test confusion matrix
# >> [[324   0]
# >>  [ 36   0]]

# 더미 모델 생성
from sklearn.dummy import DummyClassifier

dummy_model = DummyClassifier(stratigy='most_frequent')
dummy_model.fit(X_train, y_train)

# hitmap 그리기
fig, ax = plt.subplot(1, 1, figsize=(5,5))
plot_confusion_matrix(dummy_model, # 학습 모델
                      X_train,
                      y_train,
                      display_labels=['Neg', 'Pos'],
                      cmap='Blues',
                      ax=ax)
plt.show()
```
- output
- ![image](https://user-images.githubusercontent.com/77317312/112239134-40c51c80-8c89-11eb-858c-19e1e9724908.png)
#### classification_report()
```python
from sklean.metrics import classification_report
result = classification_report(y_test, pred_test_tree)
print(result) # print하면 이쁘게 나옴
# >>               precision    recall  f1-score   support
# >> 
# >>        False       0.97      0.96      0.96       324
# >>         True       0.66      0.69      0.68        36
# >> 
# >>     accuracy                           0.93       360
# >>    macro avg       0.81      0.83      0.82       360
# >> weighted avg       0.94      0.93      0.93       360
```
## 3. 재현율과 정밀도의 관계
- 이진 분류의 경우 Precision(정밀도)가 중요한 경우와 recall(재현율)이 중요한 업무가 각각 있다.

#### 재현율이 더 중요한 경우(recall)
- 실제 Positive 데이터를 Negative로 잘못 판단하면 업무상 큰 영향이 있는 경우
- **FN(false negative)** 를 낮추는데 초점을 맞춘다.
- **암환자 판정 모델**, **보험사기적발** 모델
- `애매할 때는 **positive**로 예측해` 라고 생각하자

#### 정밀도가 더 중요한 경우(precision)
- 실제 Negative 데이터를 Positive로 잘못 판단하면 업무상 큰 영향이 있는 경우
- **FP(false positive)** 를 낮추는데 초점을 맞춘다.
- **스팸메일** 판정
- `애매할 때는 **Negative**로 예측해!` 라고 생가가하자

###### ===> 정밀도와 재현율 중 한가지를 높이면 다른 하나는 낮아지는 상충관계이다.
















