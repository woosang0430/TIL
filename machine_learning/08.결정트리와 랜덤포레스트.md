# 의사결정나무(Decision Tree)
- ![image](https://user-images.githubusercontent.com/77317312/112960297-3cb06780-917f-11eb-84b0-45c0477b1999.png)
- 스무고개와 비슷한 형식의 알고리즘
- 과대적합이 발생하기 쉽다.
- 앙상블기반 알고리즘인 랜덤 포레스트와 많은 부스팅기반 앙상블 모델의 기반 알고리즘으로 사용
> ## 순도(purity) / 불순도(impurity)
> - 서로 다른 종류의 값들이 섞여 있는 비율
> - 한 종류(class)의 값이 많을 수록 순도가 높고 불순도는 낮다.

## 과대적합(Overfitting) 문제
- 모든 데이터셋이 모두 잘 분류 되어 불순도가 0이 될 때 까지 분기해 나간다.
- 과대적합을 막기 위해서는 적당한 시점에 하위노드가 더이상 생성되지 않도록 해야한다.


## 하이퍼파라미터
- 가지치기 관련 하이퍼파라미터
  - `max_depth` : 최대 깊이
  - `max_leaf_nodes` : 생성될 최대 Leaf Node 개수 제한
  - `min_samples_leaf` : 가지를 칠 최소 sample 수
- criterion(크라이티어리언 - 판단기준)
  - 불순도 계산 방식을 하이퍼파라미터
  - gini (default)
  - entropy

## Feature(컬럼) 중요도 조회
- **feature_importances_** 속성
  - 모델을 만들 때 각 feature의 중요도를 반환
  - input data에서 중요한 feature를 찾기 위해 decision tree를 이용하기도 한다.
## wine color 분류
- https://archive.ics.uci.edu/ml/datasets/Wine+Quality
- features
    - 와인 화학성분들
       > - fixed acidity : 고정 산도
       > - volatile acidity : 휘발성 산도
       > - citric acid : 시트르산
       > - residual sugar : 잔류 당분
       > - chlorides : 염화물
       > - free sulfur dioxide : 자유 이산화황
       > - total sulfur dioxide : 총 이산화황
       > - density : 밀도
       > - pH : 수소 이온 농도
       > - sulphates : 황산염
       > - alcohol : 알콜
    - quality: 와인 등급 (A>B>C)
- target - color
    - 0: white, 1: red
```python
import pandas as pd
import numpy as np

wine = pd.read_csv('data/wine.csv')
wine.head()

# data, target 분리
X = wine.drop('color', axis=1)
y = wine['color']

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

# labelencoding
X['quality'] = le.fit_transform(X['quality'])

# train/test(val) 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1) 

# gridsearch
param = dict(
    max_depth=range(2, 15),
    min_samples_leaf=[100,500,1000,2000],
    max_leaf_nodes=[5,10,20,30]
)

tree = DecisionTreeClassifier(random_state=1)

gs = GridSearchCV(tree,
                  param_grid=param,
                  scoring='accuracy',
                  cv=10,
                  n_jobs=-1)
                  
# 학습
gs.fit(X_train, y_train)
```
## Feature(컬럼) 중요도 조회
- .feature_importances_
```python
best_tree = gs.best_estimator_
fi = best_tree.feature_importances_
fi
# >> array([0.        , 0.05198967, 0.        , 0.        , 0.21920248,
# >>        0.        , 0.72668479, 0.00212306, 0.        , 0.        ,
# >>        0.        , 0.        ])

fi_s = pd.Series(fi, index=X_train.columns)
fi_s.sort_values(ascending=False)
# >> total sulfur dioxide    0.726685
# >> chlorides               0.219202
# >> volatile acidity        0.051990
# >> density                 0.002123
# >> fixed acidity           0.000000
# >> citric acid             0.000000
# >> residual sugar          0.000000
# >> free sulfur dioxide     0.000000
# >> pH                      0.000000
# >> sulphates               0.000000
# >> alcohol                 0.000000
# >> quality                 0.000000
# >> dtype: float64

import matplotlib.pyplot as plt
fi_s.sort_values().plot.barh()

plt.show()
```
- ![image](https://user-images.githubusercontent.com/77317312/112965605-6029e100-9184-11eb-89fd-67ac83194114.png)












