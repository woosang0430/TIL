# Deep Learning
- 신경망이 많아지면 깊은 딥러닝이라고 한다.
- ![image](https://user-images.githubusercontent.com/77317312/126031089-741853c2-51f5-4d0d-954c-4607c94d86e2.png)

## 1. 손실함수(Loss function, 비용함수)
------------
- Model을 통해 나온 예측값와 실제 데이터의 차이를 수치화하는 함수
- 손실함수의 값을 최소화하는 가중치(Weight)와 편향(bias)를 찾는 것이 학습의 목표
- **해결하려는 문제의 종류에 따라 표준적인 Loss function이 존재함**

| 문제 | 함수 | 설명 |
| -- | -- | -- |
| Binary classification | `binary_crossentropy` | 두 개의 클래스 분류 |
| Multi-class classification | `categorical_crossentropy` | 다중 클래스 분류 |
| Regression | `Mean Squared Error` | 연속형 값 예측 |


## 2. 활성화 함수(Activation Function)
------------
- 인공신경망에 대한 연구가 한계를 맞게된 첫 과제는 XOR문제였다.
- ![image](https://user-images.githubusercontent.com/77317312/126032001-3efd2e47-d455-466f-bbe8-eb52a8675328.png)
- AND와 OR 문제는 해결할 수 있었지만 선형 분류기라는 한계에 의해 XOR과 같은 non-linear한 문제를 해결할 수 없었다.
- 이를 해결하기 위해 활성화 함수가 나왔다.
------------
- ![image](https://user-images.githubusercontent.com/77317312/126032056-8f51153a-f7e4-4b01-8ad2-d8cbba46c20e.png)
### 활성화 함수 사용으로 입력값에 대한 출력값이 linear하게 나오지 않으므로 선형분류기를 비선형 시스템으로 만들 수 있다.
- 활성화 함수는 입력값을 non-linear한 방식으로 출력값을 도출하기 위해 사용한다.
## 주요 활성화 함수
| 활성화 함수 | 용도 | output |
| -- | -- | -- |
| `sigmoid` | 이진분류 문제의 마지막 layer에 사용 | 0 ~ 1 |
| `softmax` | 다중분류 문제의 마지막 layer에 사용 | 각 class 별 확률의 총합이 1로 반환 |
| `ReLU` | Hidden layer에 사용(대부분의 경우 기울기가 0이 되는 것을 막아준다) | 음수면 0 |

- 이렇게 활성화 함수를 이용하여 비선형 시스템인 MLP를 이용하여 XOR 해결될 수 있지만, MLP의 파라미터 개수가 증가하면서 각각의 weight와 bias를 학습시키는 것이 어렵다
- 이를 해결하기 위한 역전파


## 3. 역전파
-------------------------------------------
- 역전파 알고리즘은 출력값에 대한 입력값의 기울기(미분값)을 출력층 layer에서부터 계산하여 거꾸로 전파시키는 것이다.
- 






































[참고](https://ganghee-lee.tistory.com/30)
