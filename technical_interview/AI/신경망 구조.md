# Deep Learning
- 신경망이 많아지면 깊은 딥러닝이라고 한다.

![image](https://user-images.githubusercontent.com/77317312/126031089-741853c2-51f5-4d0d-954c-4607c94d86e2.png)

## 1. 손실함수(Loss function, 비용함수)
------------
- Model을 통해 나온 예측값와 실제 데이터의 차이를 수치화하는 함수
- 손실함수의 값을 최소화하는 가중치(Weight)와 편향(bias)를 찾는 것이 학습의 목표
- **해결하려는 문제의 종류에 따라 표준적인 Loss function이 존재함**

| 문제 | 함수 | 설명 |
| -- | -- | -- |
| Binary classification | `binary_crossentropy` | 두 개의 클래스 분류 |
| Multi-class classification | `categorical_crossentropy` | 다중 클래스 분류 |
| Regression | `Mean Squared Error` | 연속형 값 예측 |


## 2. 활성화 함수(Activation Function)
------------
- 인공신경망에 대한 연구가 한계를 맞게된 첫 과제는 XOR문제였다.

![image](https://user-images.githubusercontent.com/77317312/126032001-3efd2e47-d455-466f-bbe8-eb52a8675328.png)

- AND와 OR 문제는 해결할 수 있었지만 선형 분류기라는 한계에 의해 XOR과 같은 non-linear한 문제를 해결할 수 없었다.
- 이를 해결하기 위해 활성화 함수가 나왔다.
------------

![image](https://user-images.githubusercontent.com/77317312/126032056-8f51153a-f7e4-4b01-8ad2-d8cbba46c20e.png)

### 활성화 함수 사용으로 입력값에 대한 출력값이 linear하게 나오지 않으므로 선형분류기를 비선형 시스템으로 만들 수 있다.
- 활성화 함수는 입력값을 non-linear한 방식으로 출력값을 도출하기 위해 사용한다.
## 주요 활성화 함수
| 활성화 함수 | 용도 | output |
| -- | -- | -- |
| `sigmoid` | 이진분류 문제의 마지막 layer에 사용 | 0 ~ 1 |
| `softmax` | 다중분류 문제의 마지막 layer에 사용 | 각 class 별 확률의 총합이 1로 반환 |
| `ReLU` | Hidden layer에 사용(대부분의 경우 기울기가 0이 되는 것을 막아준다) | 음수면 0 |

- 이렇게 활성화 함수를 이용하여 비선형 시스템인 MLP를 이용하여 XOR 해결될 수 있지만, MLP의 파라미터 개수가 증가하면서 각각의 weight와 bias를 학습시키는 것이 어렵다
- 이를 해결하기 위한 역전파


## 3. 역전파
-------------------------------------------
- 역전파 알고리즘은 출력값에 대한 입력값의 기울기(미분값)를 출력층 layer에서부터 계산하여 거꾸로 전파시키는 것이다.
- 출력층 바로 전 layer에서 부터 기울기(미분값)를 계산하고 이를 점점 거꾸로 전파시키면서 전 layer들에서의 기울기와 서로 곱하는 형식으로 나아가면 최종적으로 출력층의 output에 대한 입력층에서의 input의 기울기(미분값)를 구할 수 있다.

### Gradient Descent
- 역전파 알고리즘이 해결한 문제는 파라미터 수가 많고 layer가 여러개 있을 때 Weight와 bias를 학습시키기 어려운 문제이다.
- 이는 역전파 알고리즘으로 각 layer에서 기울기 값을 구하고 그 기울기 값을 이용하여 Gradient Descent 방법으로 Weight와 bias를 update시키면서 해결된 것이다.
- **즉! layer에서 기울기 값을 구하는 이유는 Gradient Descent를 이용하여 가중치를 update하기 위함**

## 4. Optimizer
------------------------
- 손실함수를 줄여나가면서 학습하는 방법은 어떤 optimizer를 사용하느냐에 따라 달라진다.

![image](https://user-images.githubusercontent.com/77317312/126057171-08de2682-d059-4c10-821e-33b27661ee5b.png)

### 4-1. 경사 하강법(Gradient Descent)

![image](https://user-images.githubusercontent.com/77317312/126057197-32a74fea-7a56-473f-9d53-8ed932002ecb.png)

- 경사를 따라 내려가면서 Weight 업데이트
- learning rate가 너무 크면 학습시간이 짧아지나 global minimum에서 멀어질 수 있다.
- learning rate가 너무 작으면 학습시간이 오래걸리고 local minimum에 빠질 수 있다.

### 4-2. 배치 경사 하강법(batch Gradient Decent)
- batch : 가중치 등의 매개변수의 값을 조정하기 위해 사용하는 데이터 양
- 배치를 전체 데이터로 두는 것
- 1epoch당 시간은 오래 걸리고 메모리를 크게 요구하나, global minimum을 찾을 수 있다.

### 4-3. 확률적 경사 하강법(Stochastic Gradient Descent, SGD)

![image](https://user-images.githubusercontent.com/77317312/126057270-10273d4e-b08b-46fc-8b97-af4e7687ba3b.png)

- 매개변수 값 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산
- 적은 데이터 사용으로 빠르게 계산이 가능하지만, 배치 경사 하강법 보다 정확도가 낮을 수 있다.

### 4-4. 미니 배치 경사 하강법(Mini-Batch Gradient Descent)
- 정해준 데이터 양에 대해서만 계산하여 weight 조정
- 전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적이다.

### 4-5. 모멘텀(Momentum)

![image](https://user-images.githubusercontent.com/77317312/126057353-88d8c9d2-f6a6-4977-8fc7-2e8b7c33d3fe.png)

- 경사 하강법 + 관성
- 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기 값을 일정한 비율만큼 반영
- local minimum에 빠져도 관성의 힘으로 빠져 나옴

### 4-6. AdaGrad
> - 학습률을 정하는 방법으로 학습률 감소 **learning rate decay**가 있음
>   - 학습을 진행하면서 점차 학습률을 줄여나가는 방법
>   - 하지만 학습이 계속되면서 학습률이 0에 가까워져 학습이 진행되지 않는 문제가 발생
- 반면에 **AdaGrad**는 과거의 기울기 값을 제곱해서 계속 더하는 식으로 학습률을 낮춤
- 하지만 학습이 진행될수록 제곱의 값으로 학습의 정도가 크게 떨어진다.

#### 4-7 RMSProp
- AdaGrad를 개선하기 위해 **RMSProp**이 사용
- RMSProp은 과거의 모든 기울기를 균일하게 더하지 않고 새로운 기울기의 정보만 반영하도록 하여 학습률이 크게 떨어져 0에 가까워지는 것을 방지하는 방법

### 4-8 아담(Adam)
- RMSprop와 모멘텀을 합친 방법
- 제일 많이 쓰임

![image](https://user-images.githubusercontent.com/77317312/126057374-d2b43304-c59b-4b16-8105-1eeea56f5e75.png)

- Q. GD란 무엇인가?
> - 전체 데이터셋을 갖고 한 발자국 전진할 때마다 최적의 값을 찾아 weight를 update 해나가는 알고리즘

- Q. GD의 문제점은 무엇인가?
> 1. local minimum에 빠질 위험
> 2. plateau 현상
> 3. zigzag 현상

- Q. 왜 Optimizer를 쓰는가?
> - Optimizer를 사용하지 않는 경우
>   - Weight가 계속 update  되어야 하는데, local minimum에 빠져 더 이상 update 안되는 현상이 일어나거나,
>   - 초기에는 좀더 빠른 step size로 나아가도 되는데 매우 느리게 update 되는 plateau 현상이 나타나거나,
>   - Weight를 업데이트 하는데 있어 원하는 방향으로 바로 업데이트 못하고 zigzag로 나아가는 현상이 나타난다.

- Q. 왜 Optimizer로 Adam을 쓰는가?
> - GD를 update할 때 Optimization하는 방식은 크게
>   1. **스텝 방향을 최대한 일직선**으로 하거나
>   2. **스텝 속도를 최대한 빠르게 하는 방식**으로 반전해 왔는데,
> - adam은 스텝 방향과 스텝 속도 모두를 고려한 optimization이다.


[wikidiocs](https://wikidocs.net/36033)
[참고](https://ganghee-lee.tistory.com/30)
